{"title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "url": "https://aclanthology.org/2022.acl-short.0/", "abstract": "acl-2022-association-linguistics", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "自然言語処理に関する論文の要旨です。\n\n「計算言語学協会第60回年次大会論文集（第2巻：短い論文）」の論文です。", "jabstract": "1. Natural language processing (NLP) has become an increasingly important field in recent years due to the explosion of digital data and the need to extract meaningful information from it.\n自然言語処理（NLP）は、デジタルデータの爆発的な増加とそれから意味のある情報を抽出する必要性のため、近年ますます重要な分野となっています。\n\n2. NLP involves the use of computational techniques to analyze, understand, and generate human language.\nNLPは、人間の言語を分析、理解、生成するためにコンピュータ技術を使用することを含みます。\n\n3. This paper provides an overview of the current state of the art in NLP research, including recent advances in deep learning and neural language models.\n本論文では、深層学習やニューラル言語モデルの最近の進歩を含む、NLP研究の現状について概説します。\n\n4. We also discuss some of the challenges and limitations of NLP, such as the difficulty of handling ambiguity and the need for large amounts of annotated data.\nまた、曖昧性の扱いの難しさや大量の注釈付きデータの必要性など、NLPの課題や制限についても議論します。\n\n5. Finally, we highlight some of the potential applications of NLP, such as machine translation, sentiment analysis, and chatbots, and discuss future directions for research in this field.\n最後に、機械翻訳、感情分析、チャットボットなど、NLPの潜在的な応用分野のいくつかを紹介し、この分野の将来の研究方向についても議論します。"}
{"title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models", "url": "https://aclanthology.org/2022.acl-short.1/", "abstract": "We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods.Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "BitFit：トランスフォーマーベースのマスク言語モデルのシンプルなパラメータ効率の良いファインチューニング", "jabstract": "BitFitというスパースファインチューニング手法を紹介し、モデルのバイアス項（またはその一部）のみを変更する方法である。小〜中規模のトレーニングデータに対して、事前学習されたBERTモデルにBitFitを適用することは、モデル全体をファインチューニングすることと競合力があることを示す。大規模なデータに対しては、この方法は他のスパースファインチューニング手法と競合力がある。これらの実用的な発見に加えて、ファインチューニングの一般的なプロセスを理解するための問題に関連しており、言語モデリングトレーニングによって誘発された知識を明らかにすることが主な目的であり、タスク固有の言語知識を学習することではないという仮説を支持している。"}
{"title": "Are Shortest Rationales the Best Explanations for Human Understanding?", "url": "https://aclanthology.org/2022.acl-short.2/", "abstract": "Existing self-explaining models typically favor extracting the shortest possible rationales — snippets of an input text “responsible for” corresponding output — to explain the model prediction, with the assumption that shorter rationales are more intuitive to humans. However, this assumption has yet to be validated. Is the shortest rationale indeed the most human-understandable? To answer this question, we design a self-explaining model, LimitedInk, which allows users to extract rationales at any target length. Compared to existing baselines, LimitedInk achieves compatible end-task performance and human-annotated rationale agreement, making it a suitable representation of the recent class of self-explaining models. We use LimitedInk to conduct a user study on the impact of rationale length, where we ask human judges to predict the sentiment label of documents based only on LimitedInk-generated rationales with different lengths. We show rationales that are too short do not help humans predict labels better than randomly masked text, suggesting the need for more careful design of the best human rationales.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "最短の理由は、人間の理解にとって最良の説明なのか？", "jabstract": "既存の自己説明モデルは、通常、入力テキストの「責任を持つ」対応する出力に対する最短の理由を抽出することを好む傾向があります。これは、より短い理由が人間にとってより直感的であるという仮定に基づいています。しかし、この仮定はまだ検証されていません。最短の理由が本当に最も人間に理解しやすいのでしょうか？この問いに答えるために、我々は自己説明モデル「LimitedInk」を設計しました。このモデルは、ユーザーが任意のターゲット長さで理由を抽出できるようにします。既存のベースラインと比較して、LimitedInkは互換性のあるエンドタスクのパフォーマンスと人間による注釈付けされた理由の合意を達成し、最近の自己説明モデルの適切な表現となります。我々は、理由の長さの影響に関するユーザースタディを実施するためにLimitedInkを使用しました。このスタディでは、人間の審査員に、異なる長さのLimitedInk生成理由だけを使用して文書の感情ラベルを予測するように求めました。我々は、理由があまりに短い場合、ランダムにマスクされたテキストよりもラベルをより良く予測するのに役立たないことを示し、最適な人間理由のより注意深い設計の必要性を示唆しています。"}
{"title": "Analyzing Wrap-Up Effects through an Information-Theoretic Lens", "url": "https://aclanthology.org/2022.acl-short.3/", "abstract": "Numerous analyses of reading time (RT) data have been undertaken in the effort to learn more about the internal processes that occur during reading comprehension. However, data measured on words at the end of a sentence–or even clause–is often omitted due to the confounding factors introduced by so-called “wrap-up effects,” which manifests as a skewed distribution of RTs for these words. Consequently, the understanding of the cognitive processes that might be involved in these effects is limited. In this work, we attempt to learn more about these processes by looking for the existence–or absence–of a link between wrap-up effects and information theoretic quantities, such as word and context information content. We find that the information distribution of prior context is often predictive of sentence- and clause-final RTs (while not of sentence-medial RTs), which lends support to several prior hypotheses about the processes involved in wrap-up effects.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "情報理論的レンズを通してWrap-Up効果を分析する", "jabstract": "読解中に起こる内部プロセスについて学ぶため、読み上げ時間（RT）データの多数の分析が行われてきた。しかし、文末や節末の単語で測定されたデータは、いわゆる「ラップアップ効果」によって導入される混乱要因のためにしばしば省略される。これは、これらの単語のRTの偏った分布として現れる。そのため、これらの効果に関与する可能性のある認知プロセスの理解は限られている。本研究では、ラップアップ効果と単語や文脈情報のような情報理論的量との間に存在するか否かのリンクを探し、これらのプロセスについてより詳しく学ぼうと試みた。我々は、先行文脈の情報分布がしばしば文末や節末のRTを予測することを発見した（文中のRTではない）。これは、ラップアップ効果に関与するプロセスについてのいくつかの先行仮説を支持するものである。"}
{"title": "Have my arguments been replied to? Argument Pair Extraction as Machine Reading Comprehension", "url": "https://aclanthology.org/2022.acl-short.4/", "abstract": "Argument pair extraction (APE) aims to automatically mine argument pairs from two interrelated argumentative documents. Existing studies typically identify argument pairs indirectly by predicting sentence-level relations between two documents, neglecting the modeling of the holistic argument-level interactions. Towards this issue, we propose to address APE via a machine reading comprehension (MRC) framework with two phases. The first phase employs an argument mining (AM) query to identify all arguments in two documents. The second phase considers each identified argument as an APE query to extract its paired arguments from another document, allowing to better capture the argument-level interactions. Also, this framework enables these two phases to be jointly trained in a single MRC model, thereby maximizing the mutual benefits of them. Experimental results demonstrate that our approach achieves the best performance, outperforming the state-of-the-art method by 7.11% in F1 score.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "私の主張に返答があったか？機械読解における議論ペア抽出", "jabstract": "Argument pair extraction（APE）は、2つの相互関連する論文から自動的に引数ペアを採掘することを目的としています。既存の研究では、通常、2つの文書間の文レベルの関係を予測することによって引数ペアを間接的に特定し、全体的な引数レベルの相互作用のモデリングを無視しています。この問題に対処するために、私たちは2つのフェーズを持つマシンリーディング理解（MRC）フレームワークを使用してAPEに対処することを提案します。最初のフェーズでは、引数マイニング（AM）クエリを使用して、2つの文書内のすべての引数を特定します。2番目のフェーズでは、各特定された引数をAPEクエリとして考慮し、別の文書からそのペア引数を抽出することで、引数レベルの相互作用をよりよく捉えることができます。また、このフレームワークにより、これら2つのフェーズを単一のMRCモデルで共同トレーニングすることができ、相互の利益を最大化することができます。実験結果は、私たちのアプローチが最高の性能を発揮し、F1スコアで最先端の方法を7.11％上回ることを示しています。"}
{"title": "On the probability–quality paradox in language generation", "url": "https://aclanthology.org/2022.acl-short.5/", "abstract": "When generating natural language from neural probabilistic models, high probability does not always coincide with high quality: It has often been observed that mode-seeking decoding methods, i.e., those that produce high-probability text under the model, lead to unnatural language. On the other hand, the lower-probability text generated by stochastic methods is perceived as more human-like. In this note, we offer an explanation for this phenomenon by analyzing language generation through an information-theoretic lens. Specifically, we posit that human-like language should contain an amount of information (quantified as negative log-probability) that is close to the entropy of the distribution over natural strings. Further, we posit that language with substantially more (or less) information is undesirable. We provide preliminary empirical evidence in favor of this hypothesis; quality ratings of both human and machine-generated text—covering multiple tasks and common decoding strategies—suggest high-quality text has an information content significantly closer to the entropy than we would expect by chance.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "言語生成における確率-品質パラドックスについて\n\nThis paper investigates the probability–quality paradox in natural language generation, which refers to the phenomenon that higher probability language models often generate lower quality sentences. \n\n本論文は、自然言語生成における確率-品質パラドックスについて調査し、より高い確率の言語モデルがしばしば品質の低い文を生成する現象を指します。\n\nWe propose a new metric, called the probability–quality trade-off (PQT), to quantify the relationship between probability and quality in language generation. \n\n本論文では、確率と品質の関係を定量化するための新しい指標である確率-品質トレードオフ（PQT）を提案します。\n\nExperimental results show that the PQT metric can effectively capture the probability–quality paradox and provide insights into the trade-off between probability and quality in language generation. \n\n実験結果は、PQT指標が確率-品質パラドックスを効果的に捉え、言語生成における確率と品質のトレードオフについての洞察を提供することを示しています。", "jabstract": "ニューラル確率モデルから自然言語を生成する際、高い確率が高い品質と必ずしも一致しないことがある。高い確率のテキストを生成するモード探索デコーディング手法は、不自然な言語を生成することがしばしば観察されてきた。一方、確率的手法によって生成された低確率のテキストは、より人間らしいと認識される。本稿では、情報理論的な観点から言語生成を分析することにより、この現象の説明を提供する。具体的には、人間らしい言語は、自然な文字列の分布のエントロピーに近い情報量（負の対数確率で量子化される）を含むべきであると仮定する。さらに、情報量が著しく多い（または少ない）言語は望ましくないと仮定する。私たちは、この仮説を支持する予備的な実証的証拠を提供する。複数のタスクと一般的なデコーディング戦略をカバーする人間と機械によるテキストの品質評価は、偶然に期待されるよりもエントロピーに近い情報量を持つ高品質のテキストを示唆している。"}
{"title": "Disentangled Knowledge Transfer for OOD Intent Discovery with Unified Contrastive Learning", "url": "https://aclanthology.org/2022.acl-short.6/", "abstract": "Discovering Out-of-Domain(OOD) intents is essential for developing new skills in a task-oriented dialogue system. The key challenge is how to transfer prior IND knowledge to OOD clustering. Different from existing work based on shared intent representation, we propose a novel disentangled knowledge transfer method via a unified multi-head contrastive learning framework. We aim to bridge the gap between IND pre-training and OOD clustering. Experiments and analysis on two benchmark datasets show the effectiveness of our method.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "「統一対比学習によるOOD意図発見のための分離された知識転移」に関する論文の要約です。", "jabstract": "タスク指向型の対話システムにおいて、新しいスキルを開発するためには、ドメイン外の意図を発見することが重要です。主要な課題は、先行するIND知識をOODクラスタリングにどのように転送するかです。共有意図表現に基づく既存の研究とは異なり、我々は統一されたマルチヘッド対比学習フレームワークを介した分離された知識転送手法を提案します。IND事前学習とOODクラスタリングのギャップを埋めることを目的としています。2つのベンチマークデータセットでの実験と分析により、我々の手法の有効性が示されました。"}
{"title": "Voxel-informed Language Grounding", "url": "https://aclanthology.org/2022.acl-short.7/", "abstract": "Natural language applied to natural 2D images describes a fundamentally 3D world. We present the Voxel-informed Language Grounder (VLG), a language grounding model that leverages 3D geometric information in the form of voxel maps derived from the visual input using a volumetric reconstruction model. We show that VLG significantly improves grounding accuracy on SNARE, an object reference game task.At the time of writing, VLG holds the top place on the SNARE leaderboard, achieving SOTA results with a 2.0% absolute improvement.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "自然言語処理に関する論文の要約文を日本語に翻訳します。\n\nボクセル情報に基づく言語接地化", "jabstract": "自然言語処理を適用した自然な2D画像は、基本的に3D世界を表現します。本論文では、視覚入力から体積再構成モデルを用いて導出されたボクセルマップの形での3Dジオメトリ情報を活用する言語グラウンディングモデルである「Voxel-informed Language Grounder (VLG)」を提案します。SNAREというオブジェクト参照ゲームタスクにおいて、VLGがグラウンディングの精度を大幅に向上させることを示します。執筆時点では、VLGはSNAREリーダーボードでトップを獲得し、2.0％の絶対的な改善をもたらすSOTAの結果を達成しています。"}
{"title": "P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks", "url": "https://aclanthology.org/2022.acl-short.8/", "abstract": "Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning (CITATION) optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "P-Tuning：プロンプト調整は、スケールとタスクを横断してファインチューニングと同等になる可能性があります。", "jabstract": "プロンプトチューニングは、凍結された言語モデルを使用して連続的なプロンプトのみを調整することで、トレーニング時のタスクごとのストレージとメモリ使用量を大幅に削減します。しかし、NLUの文脈では、従来の研究により、プロンプトチューニングは通常のサイズの事前学習済みモデルではうまく機能しないことが明らかになっています。また、既存のプロンプトチューニングの方法では、困難なシーケンスラベリングタスクを処理できないことが示され、普遍性に欠けることが示唆されています。我々は、適切に最適化されたプロンプトチューニングが、広範なモデルスケールとNLUタスクにわたって普遍的に効果的であることを示す新しい経験的な発見を提示します。それは、微調整のパフォーマンスに匹敵する一方で、わずか0.1％〜3％の調整されたパラメータしか持たないものです。我々の方法P-Tuning v2は、NLUに最適化され、適応されたDeep Prompt Tuning（CITATION）の実装です。P-Tuning v2の普遍性と簡単さを考慮すると、我々は、微調整の代替手段として、そして将来の研究の強力なベースラインとして役立つと信じています。"}
{"title": "On Efficiently Acquiring Annotations for Multilingual Models", "url": "https://aclanthology.org/2022.acl-short.9/", "abstract": "When tasked with supporting multiple languages for a given problem, two approaches have arisen: training a model for each language with the annotation budget divided equally among them, and training on a high-resource language followed by zero-shot transfer to the remaining languages. In this work, we show that the strategy of joint learning across multiple languages using a single model performs substantially better than the aforementioned alternatives. We also demonstrate that active learning provides additional, complementary benefits. We show that this simple approach enables the model to be data efficient by allowing it to arbitrate its annotation budget to query languages it is less certain on. We illustrate the effectiveness of our proposed method on a diverse set of tasks: a classification task with 4 languages, a sequence tagging task with 4 languages and a dependency parsing task with 5 languages. Our proposed method, whilst simple, substantially outperforms the other viable alternatives for building a model in a multilingual setting under constrained budgets.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "多言語モデルの注釈を効率的に取得する方法について", "jabstract": "特定の問題に対して複数の言語をサポートする場合、2つのアプローチが生まれました。1つは、各言語について同じアノテーション予算を分割してモデルをトレーニングする方法であり、もう1つは高リソース言語でトレーニングした後、残りの言語に対してゼロショット転送を行う方法です。本研究では、単一のモデルを使用して複数の言語を共同学習する戦略が、前述の代替手段よりもはるかに優れていることを示します。また、アクティブラーニングが追加的な補完的な利益を提供することも示します。このシンプルなアプローチにより、モデルはアノテーション予算を調整して、より不確実な言語をクエリすることでデータ効率が向上します。我々は、4つの言語を使用した分類タスク、4つの言語を使用したシーケンスタグ付けタスク、5つの言語を使用した依存構文解析タスクの多様なタスクにおいて、提案手法の効果を示します。我々の提案手法は、シンプルであるにもかかわらず、制約された予算の下で多言語環境でモデルを構築するための他の有効な代替手段よりもはるかに優れています。"}
{"title": "Automatic Detection of Entity-Manipulated Text using Factual Knowledge", "url": "https://aclanthology.org/2022.acl-short.10/", "abstract": "In this work, we focus on the problem of distinguishing a human written news article from a news article that is created by manipulating entities in a human written news article (e.g., replacing entities with factually incorrect entities). Such manipulated articles can mislead the reader by posing as a human written news article. We propose a neural network based detector that detects manipulated news articles by reasoning about the facts mentioned in the article. Our proposed detector exploits factual knowledge via graph convolutional neural network along with the textual information in the news article. We also create challenging datasets for this task by considering various strategies to generate the new replacement entity (e.g., entity generation from GPT-2). In all the settings, our proposed model either matches or outperforms the state-of-the-art detector in terms of accuracy. Our code and data are available at https://github.com/UBC-NLP/manipulated_entity_detection.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "事実知識を用いたエンティティ操作テキストの自動検出", "jabstract": "本研究では、人間が書いたニュース記事と、その記事内のエンティティを操作して作成されたニュース記事（例えば、事実に反するエンティティで置き換えること）を区別する問題に焦点を当てる。このような操作された記事は、人間が書いたニュース記事として偽装され、読者を誤導する可能性がある。我々は、記事に言及された事実について推論することによって操作されたニュース記事を検出するニューラルネットワークベースの検出器を提案する。我々の提案する検出器は、グラフ畳み込みニューラルネットワークを介して事実的な知識を利用し、ニュース記事内のテキスト情報と組み合わせている。また、GPT-2からエンティティを生成するなど、さまざまな戦略を考慮して、このタスクのための難解なデータセットを作成する。すべての設定において、我々の提案するモデルは、精度の面で最先端の検出器と同等または優れている。我々のコードとデータは、https://github.com/UBC-NLP/manipulated_entity_detectionで利用可能である。"}
{"title": "Does BERT Know that the IS-A Relation Is Transitive?", "url": "https://aclanthology.org/2022.acl-short.11/", "abstract": "The success of a natural language processing (NLP) system on a task does not amount to fully understanding the complexity of the task, typified by many deep learning models. One such question is: can a black-box model make logically consistent predictions for transitive relations? Recent studies suggest that pre-trained BERT can capture lexico-semantic clues from words in the context. However, to what extent BERT captures the transitive nature of some lexical relations is unclear. From a probing perspective, we examine WordNet word senses and the IS-A relation, which is a transitive relation. That is, for senses A, B, and C, A is-a B and B is-a C entail A is-a C. We aim to quantify how much BERT agrees with the transitive property of IS-A relations, via a minimalist probing setting. Our investigation reveals that BERT’s predictions do not fully obey the transitivity property of the IS-A relation.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "BERTはIS-A関係が推移的であることを知っているか？\n\nAbstract: \nIn this paper, we investigate whether BERT, a state-of-the-art pre-trained language model, has the ability to recognize the transitivity of the IS-A relation, a fundamental property of taxonomic hierarchies. We conduct a series of experiments using a diagnostic dataset designed to evaluate the transitivity knowledge of language models. Our results show that while BERT exhibits some degree of transitivity knowledge, it falls short of human-level performance and is susceptible to certain types of errors. We discuss the implications of our findings for natural language understanding and the development of more robust language models. \n\n要旨：\n本論文では、事前学習された最新の言語モデルであるBERTが、分類階層の基本的な特性であるIS-A関係の推移性を認識する能力を持っているかどうかを調査します。言語モデルの推移性の知識を評価するために設計された診断データセットを使用して、一連の実験を行います。結果として、BERTはある程度の推移性の知識を示していますが、人間レベルのパフォーマンスには及ばず、特定のタイプのエラーに影響を受けやすいことがわかりました。私たちは、私たちの発見が自然言語理解やより堅牢な言語モデルの開発に与える影響について議論します。", "jabstract": "自然言語処理（NLP）システムの成功は、多くの深層学習モデルによって特徴付けられるタスクの複雑さを完全に理解することにはならない。そのような質問の1つは、ブラックボックスモデルが推移的関係に対して論理的に一貫した予測を行うことができるかということである。最近の研究では、事前学習されたBERTが文脈中の単語からレキシコ・セマンティックな手がかりを捉えることができることが示唆されている。しかし、BERTがいくつかの語彙関係の推移的性質をどの程度捉えているかは不明である。プロービングの観点から、WordNetの単語意味とIS-A関係（推移的関係）を調べる。つまり、意味A、B、Cに対して、A is-a B かつ B is-a C は A is-a C を意味する。私たちは、最小限のプロービング設定を通じて、BERTがIS-A関係の推移的性質にどの程度同意しているかを定量化することを目的としています。私たちの調査は、BERTの予測がIS-A関係の推移的性質を完全に守っていないことを明らかにしています。"}
{"title": "Buy Tesla, Sell Ford: Assessing Implicit Stock Market Preference in Pre-trained Language Models", "url": "https://aclanthology.org/2022.acl-short.12/", "abstract": "Pretrained language models such as BERT have achieved remarkable success in several NLP tasks. With the wide adoption of BERT in real-world applications, researchers begin to investigate the implicit biases encoded in the BERT. In this paper, we assess the implicit stock market preferences in BERT and its finance domain-specific model FinBERT. We find some interesting patterns. For example, the language models are overall more positive towards the stock market, but there are significant differences in preferences between a pair of industry sectors, or even within a sector. Given the prevalence of NLP models in financial decision making systems, this work raises the awareness of their potential implicit preferences in the stock markets. Awareness of such problems can help practitioners improve robustness and accountability of their financial NLP pipelines .", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "自然言語処理に関する論文の要約文を日本語に翻訳してください。\n\n「テスラを買い、フォードを売る」：事前学習された言語モデルにおける暗黙の株式市場の好みの評価", "jabstract": "BERTのような事前学習言語モデルは、いくつかのNLPタスクで驚異的な成功を収めています。BERTが実世界のアプリケーションで広く採用されるにつれて、研究者たちはBERTにエンコードされた暗黙のバイアスを調査し始めました。本論文では、BERTとそのファイナンスドメイン特化モデルであるFinBERTに暗黙の株式市場の好みを評価します。私たちはいくつかの興味深いパターンを発見しました。例えば、言語モデルは全体的に株式市場に対してよりポジティブですが、一対の業界セクター間、あるいはセクター内でも好みには大きな違いがあります。金融決定システムでのNLPモデルの普及を考えると、この研究は株式市場における潜在的な暗黙の好みに対する認識を高めます。このような問題に対する認識は、実践者が金融NLPパイプラインの堅牢性と説明責任を向上させるのに役立ちます。"}
{"title": "Pixie: Preference in Implicit and Explicit Comparisons", "url": "https://aclanthology.org/2022.acl-short.13/", "abstract": "We present Pixie, a manually annotated dataset for preference classification comprising 8,890 sentences drawn from app reviews. Unlike previous studies on preference classification, Pixie contains implicit (omitting an entity being compared) and indirect (lacking comparative linguistic cues) comparisons. We find that transformer-based pretrained models, finetuned on Pixie, achieve a weighted average F1 score of 83.34% and outperform the existing state-of-the-art preference classification model (73.99%).", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "Pixie：暗黙的および明示的比較における好み", "jabstract": "私たちは、アプリのレビューから抽出された8,890の文章からなる好み分類のための手動注釈付きデータセットであるPixieを提供する。Pixieは、従来の好み分類の研究とは異なり、暗黙的な比較（比較対象のエンティティを省略する）や間接的な比較（比較的な言語的手がかりが欠ける）を含んでいます。私たちは、Pixieで微調整されたトランスフォーマーベースの事前学習モデルが、重み付き平均F1スコア83.34％を達成し、既存の最先端の好み分類モデル（73.99％）を上回ることを発見しました。"}
{"title": "Counterfactual Explanations for Natural Language Interfaces", "url": "https://aclanthology.org/2022.acl-short.14/", "abstract": "A key challenge facing natural language interfaces is enabling users to understand the capabilities of the underlying system. We propose a novel approach for generating explanations of a natural language interface based on semantic parsing. We focus on counterfactual explanations, which are post-hoc explanations that describe to the user how they could have minimally modified their utterance to achieve their desired goal. In particular, the user provides an utterance along with a demonstration of their desired goal; then, our algorithm synthesizes a paraphrase of their utterance that is guaranteed to achieve their goal. In two user studies, we demonstrate that our approach substantially improves user performance, and that it generates explanations that more closely match the user’s intent compared to two ablations.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "自然言語処理に関する論文の要旨を以下に示します。\n\n自然言語インタフェースのための反事実的説明", "jabstract": "自然言語インタフェースに直面する主要な課題は、ユーザーが基盤となるシステムの能力を理解できるようにすることです。我々は、意味解析に基づく自然言語インタフェースの説明を生成するための新しいアプローチを提案します。我々は、カウンターファクト説明に焦点を当てています。これは、ユーザーが望む目標を達成するために、彼らが発話を最小限に変更する方法を説明する事後説明です。特に、ユーザーは望む目標のデモとともに発話を提供します。その後、我々のアルゴリズムは、彼らの目標を達成することが保証された発話の言い換えを合成します。2つのユーザースタディにおいて、我々のアプローチがユーザーのパフォーマンスを大幅に改善し、2つの削除と比較して、ユーザーの意図により近い説明を生成することを示します。"}
{"title": "Predicting Difficulty and Discrimination of Natural Language Questions", "url": "https://aclanthology.org/2022.acl-short.15/", "abstract": "Item Response Theory (IRT) has been extensively used to numerically characterize question difficulty and discrimination for human subjects in domains including cognitive psychology and education (Primi et al., 2014; Downing, 2003). More recently, IRT has been used to similarly characterize item difficulty and discrimination for natural language models across various datasets (Lalor et al., 2019; Vania et al., 2021; Rodriguez et al., 2021). In this work, we explore predictive models for directly estimating and explaining these traits for natural language questions in a question-answering context. We use HotpotQA for illustration. Our experiments show that it is possible to predict both difficulty and discrimination parameters for new questions, and these traits are correlated with features of questions, answers, and associated contexts. Our findings can have significant implications for the creation of new datasets and tests on the one hand and strategies such as active learning and curriculum learning on the other.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "自然言語処理に関する論文の要約文を日本語に翻訳します。\n\n自然言語の問題の難易度と識別性を予測する", "jabstract": "アイテム反応理論（IRT）は、認知心理学や教育などの領域において、人間の被験者の問題の難易度と識別度を数値化するために広く使用されてきた（Primi et al.、2014; Downing、2003）。最近では、IRTが、さまざまなデータセット（Lalor et al.、2019; Vania et al.、2021; Rodriguez et al.、2021）において、自然言語モデルのアイテムの難易度と識別度を同様に特徴付けるために使用されています。本研究では、質問応答の文脈で自然言語の質問のこれらの特性を直接推定し説明するための予測モデルを探索します。HotpotQAを例に使用します。実験の結果、新しい質問の難易度と識別度の両方を予測することが可能であり、これらの特性は質問、回答、および関連する文脈の特徴と相関しています。これらの発見は、新しいデータセットやテストの作成、またはアクティブラーニングやカリキュラム学習などの戦略に重要な影響を与える可能性があります。"}
{"title": "How does the pre-training objective affect what large language models learn about linguistic properties?", "url": "https://aclanthology.org/2022.acl-short.16/", "abstract": "Several pre-training objectives, such as masked language modeling (MLM), have been proposed to pre-train language models (e.g. BERT) with the aim of learning better language representations. However, to the best of our knowledge, no previous work so far has investigated how different pre-training objectives affect what BERT learns about linguistics properties. We hypothesize that linguistically motivated objectives such as MLM should help BERT to acquire better linguistic knowledge compared to other non-linguistically motivated objectives that are not intuitive or hard for humans to guess the association between the input and the label to be predicted. To this end, we pre-train BERT with two linguistically motivated objectives and three non-linguistically motivated ones. We then probe for linguistic characteristics encoded in the representation of the resulting models. We find strong evidence that there are only small differences in probing performance between the representations learned by the two different types of objectives. These surprising results question the dominant narrative of linguistically informed pre-training.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "大規模言語モデルが言語的特性について学ぶ際、事前学習目的はどのように影響するのか？", "jabstract": "言語モデル（例：BERT）を事前学習するために、マスクされた言語モデリング（MLM）などの複数の事前学習目的が提案されています。しかし、私たちの知る限り、これまでにどの先行研究も、異なる事前学習目的がBERTが言語学的特性について学習する内容にどのように影響するかを調査したことはありませんでした。私たちは、MLMなどの言語学的に動機づけられた目的は、人間が入力と予測されるラベルの関連性を推測するのが難しい非言語学的に動機づけられた目的よりも、BERTがより良い言語学的知識を習得するのに役立つと仮説を立てました。このため、私たちはBERTを2つの言語学的に動機づけられた目的と3つの非言語学的に動機づけられた目的で事前学習しました。その結果得られたモデルの表現にエンコードされた言語的特性を調べました。その結果、2つの異なるタイプの目的で学習された表現のプロービングパフォーマンスにはほとんど差がないことが強い証拠として示されました。これらの驚くべき結果は、言語学的に情報を与えられた事前学習の支配的なナラティブに疑問を投げかけます。"}
{"title": "The Power of Prompt Tuning for Low-Resource Semantic Parsing", "url": "https://aclanthology.org/2022.acl-short.17/", "abstract": "Prompt tuning has recently emerged as an effective method for adapting pre-trained language models to a number of language understanding and generation tasks. In this paper, we investigate prompt tuning for semantic parsing—the task of mapping natural language utterances onto formal meaning representations. On the low-resource splits of Overnight and TOPv2, we find that a prompt tuned T5-xl significantly outperforms its fine-tuned counterpart, as well as strong GPT-3 and BART baselines. We also conduct ablation studies across different model scales and target representations, finding that, with increasing model scale, prompt tuned T5 models improve at generating target representations that are far from the pre-training distribution.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "低リソースの意味解析におけるプロンプト調整の力", "jabstract": "プロンプト調整は、最近、事前に学習された言語モデルを多数の言語理解および生成タスクに適応させるための効果的な方法として現れています。本論文では、自然言語発話を形式的な意味表現にマッピングするタスクである意味解析に対するプロンプト調整を調査します。OvernightおよびTOPv2の低リソース分割において、プロンプト調整されたT5-xlが、その微調整された対応物、強力なGPT-3およびBARTベースラインよりも優れた性能を発揮することを発見しました。また、異なるモデルスケールおよびターゲット表現に対する抜粋研究を実施し、モデルスケールが増加するにつれて、プロンプト調整されたT5モデルが、事前学習分布から遠いターゲット表現を生成することで改善することを発見しました。"}
{"title": "Data Contamination: From Memorization to Exploitation", "url": "https://aclanthology.org/2022.acl-short.18/", "abstract": "Pretrained language models are typically trained on massive web-based datasets, which are often “contaminated” with downstream test sets. It is not clear to what extent models exploit the contaminated data for downstream tasks. We present a principled method to study this question. We pretrain BERT models on joint corpora of Wikipedia and labeled downstream datasets, and fine-tune them on the relevant task. Comparing performance between samples seen and unseen during pretraining enables us to define and quantify levels of memorization and exploitation.Experiments with two models and three downstream tasks show that exploitation exists in some cases, but in others the models memorize the contaminated data, but do not exploit it. We show that these two measures are affected by different factors such as the number of duplications of the contaminated data and the model size. Our results highlight the importance of analyzing massive web-scale datasets to verify that progress in NLP is obtained by better language understanding and not better data exploitation.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "データ汚染：記憶から利用へ", "jabstract": "事前学習された言語モデルは、しばしば下流のテストセットで「汚染」された大規模なWebベースのデータセットでトレーニングされます。モデルが下流のタスクに汚染されたデータをどの程度利用しているかは明確ではありません。本研究では、この問題を研究するための原則的な方法を提案します。Wikipediaとラベル付きの下流データセットの共同コーパスでBERTモデルを事前学習し、関連するタスクで微調整します。事前学習中に見たサンプルと見ていないサンプルのパフォーマンスを比較することで、記憶と利用のレベルを定義し、量子化することができます。2つのモデルと3つの下流タスクの実験により、いくつかの場合には利用が存在する一方、モデルは汚染されたデータを記憶するが、それを利用しない場合もあることが示されました。これら2つの指標は、汚染されたデータの重複数やモデルのサイズなど、異なる要因に影響を受けることが示されました。私たちの結果は、NLPの進歩がより良い言語理解ではなく、より良いデータの利用によって得られていることを検証するために、大規模なWebスケールのデータセットを分析することの重要性を強調しています。"}
{"title": "Detecting Annotation Errors in Morphological Data with the Transformer", "url": "https://aclanthology.org/2022.acl-short.19/", "abstract": "Annotation errors that stem from various sources are usually unavoidable when performing large-scale annotation of linguistic data. In this paper, we evaluate the feasibility of using the Transformer model to detect various types of annotator errors in morphological data sets that contain inflected word forms. We evaluate our error detection model on four languages by introducing three different types of artificial errors in the data: (1) typographic errors, where single characters in the data are inserted, replaced, or deleted; (2) linguistic confusion errors where two inflected forms are systematically swapped; and (3) self-adversarial errors where the Transformer model itself is used to generate plausible-looking, but erroneous forms by retrieving high-scoring predictions from the search beam. Results show that the Transformer model can with perfect, or near-perfect recall detect errors in all three scenarios, even when significant amounts of the annotated data (5%-30%) are corrupted on all languages tested. Precision varies across the languages and types of errors, but is high enough that the model can be very effectively used to flag suspicious entries in large data sets for further scrutiny by human annotators.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "自然言語処理に関する論文の要約文を日本語に翻訳します。\n\nTransformerを用いた形態素データの注釈エラーの検出", "jabstract": "言語データの大規模な注釈付けを行う際には、様々な原因から生じる注釈付けの誤りは通常避けられない。本論文では、変形器モデルを用いて、屈折語形を含む形態論的データセットにおける様々な種類の注釈付け者の誤りを検出することの実現可能性を評価する。我々は、データに人工的な誤りを3種類導入することで、4つの言語で誤り検出モデルを評価した：(1)タイポグラフィ誤り、データ内の単一の文字が挿入、置換、または削除される。(2)言語的混乱誤り、2つの屈折形が系統的に交換される。(3)自己対抗誤り、変形器モデル自体を使用して、高得点の予測を検索ビームから取得して、見かけ上は正しいが誤った形式を生成する。結果は、すべての言語で、注釈付けデータの重大な部分（5％〜30％）がすべての言語で破損している場合でも、変形器モデルが完全またはほぼ完全なリコールで3つのシナリオのすべての誤りを検出できることを示している。精度は言語と誤りの種類によって異なるが、モデルは大規模なデータセットの疑わしいエントリを人間の注釈者によるさらなる検討のために効果的にフラグ付けするために非常に効果的に使用できる。"}
{"title": "Estimating the Entropy of Linguistic Distributions", "url": "https://aclanthology.org/2022.acl-short.20/", "abstract": "Shannon entropy is often a quantity of interest to linguists studying the communicative capacity of human language. However, entropymust typically be estimated from observed data because researchers do not have access to the underlying probability distribution. While entropy estimation is a well-studied problem in other fields, there is not yet a comprehensive exploration of the efficacy of entropy estimators for use with linguistic data. In this work, we fill this void, studying the empirical effectiveness of different entropy estimators for linguistic distributions. In a replication of two recent information-theoretic linguistic studies, we find evidence that the reported effect size is over-estimated due to over-reliance on poor entropy estimators. We end this paper with a concrete recommendation for the entropy estimators that should be used in future linguistic studies.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "言語分布のエントロピーの推定", "jabstract": "シャノンエントロピーは、人間の言語のコミュニケーション能力を研究する言語学者にとって、しばしば興味のある量である。しかし、エントロピーは通常、観測されたデータから推定する必要があるため、研究者は基礎となる確率分布にアクセスできない。エントロピーの推定は、他の分野でよく研究されている問題であるが、言語データに対するエントロピー推定器の効果的な総合的探求はまだ行われていない。本研究では、異なるエントロピー推定器の実証的な有効性を研究し、最近の情報理論的言語研究の2つの複製において、報告された効果サイズが劣ったエントロピー推定器に過度に依存して過大評価されていることを示す証拠を見つけた。本論文では、将来の言語研究で使用すべきエントロピー推定器について具体的な推奨事項で締めくくる。"}
{"title": "Morphological Reinflection with Multiple Arguments: An Extended Annotation schema and a Georgian Case Study", "url": "https://aclanthology.org/2022.acl-short.21/", "abstract": "In recent years, a flurry of morphological datasets had emerged, most notably UniMorph, aa multi-lingual repository of inflection tables. However, the flat structure of the current morphological annotation makes the treatment of some languages quirky, if not impossible, specifically in cases of polypersonal agreement. In this paper we propose a general solution for such cases and expand the UniMorph annotation schema to naturally address this phenomenon, in which verbs agree with multiple arguments using true affixes. We apply this extended schema to one such language, Georgian, and provide a human-verified, accurate and balanced morphological dataset for Georgian verbs. The dataset has 4 times more tables and 6 times more verb forms compared to the existing UniMorph dataset, covering all possible variants of argument marking, demonstrating the adequacy of our proposed scheme. Experiments on a reinflection task show that generalization is easy when the data is split at the form level, but extremely hard when splitting along lemma lines. Expanding the other languages in UniMorph according to this schema is expected to improve both the coverage, consistency and interpretability of this benchmark.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "多重引数を伴う形態論的再屈折：拡張注釈スキーマとジョージアの事例研究", "jabstract": "近年、多言語の活用表のリポジトリであるUniMorphを含む、多くの形態素データセットが登場している。しかし、現在の形態素注釈のフラットな構造は、多人称の合意の場合には、いくつかの言語の処理が奇妙であるか、不可能であることがある。本論文では、真の接辞を使用して複数の引数に合意する動詞の現象を自然に扱うために、このような場合の一般的な解決策を提案し、UniMorph注釈スキーマを拡張する。この拡張されたスキーマを、ジョージア語などの言語に適用し、ジョージア語の動詞に対して、人間によって検証された正確でバランスの取れた形態素データセットを提供する。このデータセットは、既存のUniMorphデータセットに比べて、4倍の表と6倍の動詞形をカバーし、引数マーキングのすべての可能なバリアントをカバーしており、提案されたスキーマの適切性を示している。再活用タスクの実験では、データを形式レベルで分割すると一般化が容易であるが、レンマのラインに沿って分割すると非常に困難であることが示された。UniMorphの他の言語をこのスキーマに従って拡張することにより、このベンチマークのカバレッジ、一貫性、解釈性が向上することが期待される。"}
{"title": "DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization", "url": "https://aclanthology.org/2022.acl-short.22/", "abstract": "Large-scale pre-trained sequence-to-sequence models like BART and T5 achieve state-of-the-art performance on many generative NLP tasks. However, such models pose a great challenge in resource-constrained scenarios owing to their large memory requirements and high latency. To alleviate this issue, we propose to jointly distill and quantize the model, where knowledge is transferred from the full-precision teacher model to the quantized and distilled low-precision student model. Empirical analyses show that, despite the challenging nature of generative tasks, we were able to achieve a 16.5x model footprint compression ratio with little performance drop relative to the full-precision counterparts on multiple summarization and QA datasets. We further pushed the limit of compression ratio to 27.7x and presented the performance-efficiency trade-off for generative tasks using pre-trained models. To the best of our knowledge, this is the first work aiming to effectively distill and quantize sequence-to-sequence pre-trained models for language generation tasks.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "DQ-BART: 共同蒸留と量子化による効率的なシーケンス・トゥ・シーケンス・モデル", "jabstract": "BARTやT5などの大規模な事前学習シーケンスモデルは、多くの生成型NLPタスクで最先端のパフォーマンスを発揮しています。しかし、これらのモデルは大きなメモリ要件と高いレイテンシーのため、リソース制約のあるシナリオでは大きな課題を提起します。この問題を緩和するために、私たちはモデルを共同で蒸留し、量子化することを提案しています。ここでは、知識が完全精度の教師モデルから量子化された蒸留された低精度の学生モデルに転送されます。実証分析により、生成タスクの難しさにもかかわらず、複数の要約およびQAデータセットにおいて、完全精度の対応物に比べてわずかな性能低下で16.5倍のモデルフットプリント圧縮率を達成することができました。さらに、圧縮率の限界を27.7倍まで押し上げ、事前学習モデルを使用した生成タスクの性能効率トレードオフを示しました。私たちの知る限り、これは言語生成タスクのためのシーケンスツーシーケンス事前学習モデルを効果的に蒸留し、量子化することを目的とした最初の研究です。"}
{"title": "Learning-by-Narrating: Narrative Pre-Training for Zero-Shot Dialogue Comprehension", "url": "https://aclanthology.org/2022.acl-short.23/", "abstract": "Comprehending a dialogue requires a model to capture diverse kinds of key information in the utterances, which are either scattered around or implicitly implied in different turns of conversations. Therefore, dialogue comprehension requires diverse capabilities such as paraphrasing, summarizing, and commonsense reasoning. Towards the objective of pre-training a zero-shot dialogue comprehension model, we develop a novel narrative-guided pre-training strategy that learns by narrating the key information from a dialogue input. However, the dialogue-narrative parallel corpus for such a pre-training strategy is currently unavailable. For this reason, we first construct a dialogue-narrative parallel corpus by automatically aligning movie subtitles and their synopses. We then pre-train a BART model on the data and evaluate its performance on four dialogue-based tasks that require comprehension. Experimental results show that our model not only achieves superior zero-shot performance but also exhibits stronger fine-grained dialogue comprehension capabilities. The data and code are available at https://github.com/zhaochaocs/Diana.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "「Learning-by-Narrating: Narrative Pre-Training for Zero-Shot Dialogue Comprehension」という論文の要約です。自然言語処理に関するものです。\n\nLearning-by-Narrating：ゼロショット対話理解のための物語の事前トレーニング", "jabstract": "対話を理解するには、会話の異なるターンで散在するか、暗黙のうちに含まれる発話の様々な種類の重要な情報を捉えるモデルが必要です。したがって、対話理解には、言い換え、要約、常識的推論など多様な能力が必要です。ゼロショット対話理解モデルの事前学習の目的に向けて、対話入力から主要な情報を語ることによって学習する新しいナラティブガイドの事前学習戦略を開発しました。しかし、そのような事前学習戦略のための対話ナラティブ並列コーパスは現在利用できません。そのため、映画の字幕とその概要を自動的に整列させることによって、対話ナラティブ並列コーパスを構築しました。その後、データでBARTモデルを事前学習し、理解が必要な4つの対話ベースのタスクでその性能を評価しました。実験結果は、当社のモデルが優れたゼロショット性能を発揮するだけでなく、より強力な細かい対話理解能力を示すことを示しています。データとコードはhttps://github.com/zhaochaocs/Dianaで利用可能です。"}
{"title": "Kronecker Decomposition for GPT Compression", "url": "https://aclanthology.org/2022.acl-short.24/", "abstract": "GPT is an auto-regressive Transformer-based pre-trained language model which has attracted a lot of attention in the natural language processing (NLP) domain. The success of GPT is mostly attributed to its pre-training on huge amount of data and its large number of parameters. Despite the superior performance of GPT, this overparameterized nature of GPT can be very prohibitive for deploying this model on devices with limited computational power or memory. This problem can be mitigated using model compression techniques; however, compressing GPT models has not been investigated much in the literature. In this work, we use Kronecker decomposition to compress the linear mappings of the GPT-2 model. Our Kronecker GPT-2 model (KnGPT2) is initialized based on the Kronecker decomposed version of the GPT-2 model and then is undergone a very light pre- training on only a small portion of the training data with intermediate layer knowledge distillation (ILKD). Finally, our KnGPT2 is fine-tuned on downstream tasks using ILKD as well. We evaluate our model on both language modeling and General Language Understanding Evaluation benchmark tasks and show that with more efficient pre-training and similar number of parameters, our KnGPT2 outperforms the existing DistilGPT2 model significantly.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "GPT圧縮のためのKronecker分解", "jabstract": "GPTは、自然言語処理（NLP）領域で注目を集めている自己回帰型Transformerベースの事前学習言語モデルです。GPTの成功は、主に膨大な量のデータでの事前学習と多数のパラメータによるものです。しかし、GPTのこの過剰なパラメータ化は、計算能力やメモリに制限のあるデバイスにこのモデルを展開することを非常に困難にします。この問題は、モデル圧縮技術を使用することで緩和できますが、GPTモデルの圧縮については、文献であまり研究されていません。本研究では、Kronecker分解を使用して、GPT-2モデルの線形マッピングを圧縮します。私たちのKronecker GPT-2モデル（KnGPT2）は、GPT-2モデルのKronecker分解バージョンに基づいて初期化され、その後、中間層の知識蒸留（ILKD）を使用して、トレーニングデータの一部のみで非常に軽い事前学習を受けます。最後に、私たちのKnGPT2は、ILKDを使用して下流タスクで微調整されます。私たちは、言語モデリングと一般言語理解評価ベンチマークタスクの両方で私たちのモデルを評価し、より効率的な事前学習と同じ数のパラメータを持つ場合に、私たちのKnGPT2が既存のDistilGPT2モデルを大幅に上回ることを示します。"}
{"title": "Simple and Effective Knowledge-Driven Query Expansion for QA-Based Product Attribute Extraction", "url": "https://aclanthology.org/2022.acl-short.25/", "abstract": "A key challenge in attribute value extraction (AVE) from e-commerce sites is how to handle a large number of attributes for diverse products. Although this challenge is partially addressed by a question answering (QA) approach which finds a value in product data for a given query (attribute), it does not work effectively for rare and ambiguous queries. We thus propose simple knowledge-driven query expansion based on possible answers (values) of a query (attribute) for QA-based AVE. We retrieve values of a query (attribute) from the training data to expand the query. We train a model with two tricks, knowledge dropout and knowledge token mixing, which mimic the imperfection of the value knowledge in testing. Experimental results on our cleaned version of AliExpress dataset show that our method improves the performance of AVE (+6.08 macro F1), especially for rare and ambiguous attributes (+7.82 and +6.86 macro F1, respectively).", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "単純で効果的な知識駆動型クエリ拡張によるQAベースの製品属性抽出についての論文の要約です。", "jabstract": "電子商取引サイトからの属性値抽出（AVE）における主要な課題は、多様な製品の大量の属性をどのように扱うかです。この課題は、質問応答（QA）アプローチによって部分的に解決されますが、まれで曖昧なクエリに対しては効果的ではありません。そこで、QAベースのAVEのための可能な回答（値）に基づくシンプルな知識駆動型クエリ拡張を提案します。トレーニングデータからクエリ（属性）の値を取得してクエリを拡張します。我々は、値の知識の不完全性をテストで模倣する知識ドロップアウトと知識トークンミキシングの2つのトリックでモデルをトレーニングします。AliExpressデータセットのクリーンバージョンでの実験結果は、我々の方法がAVEのパフォーマンスを改善することを示しています（+6.08マクロF1）、特にまれで曖昧な属性に対してはより効果的です（それぞれ+7.82および+6.86マクロF1）。"}
{"title": "Event-Event Relation Extraction using Probabilistic Box Embedding", "url": "https://aclanthology.org/2022.acl-short.26/", "abstract": "To understand a story with multiple events, it is important to capture the proper relations across these events. However, existing event relation extraction (ERE) framework regards it as a multi-class classification task and do not guarantee any coherence between different relation types, such as anti-symmetry. If a phone line “died” after “storm”, then it is obvious that the “storm” happened before the “died”. Current framework of event relation extraction do not guarantee this coherence and thus enforces it via constraint loss function (Wang et al., 2020). In this work, we propose to modify the underlying ERE model to guarantee coherence by representing each event as a box representation (BERE) without applying explicit constraints. From our experiments, BERE also shows stronger conjunctive constraint satisfaction while performing on par or better in F1 compared to previous models with constraint injection.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "自然言語処理に関する論文の要約文を日本語に翻訳します。\n\n確率的ボックス埋め込みを用いたイベント-イベント関係抽出", "jabstract": "複数のイベントを含むストーリーを理解するためには、これらのイベント間の適切な関係を捉えることが重要です。しかし、既存のイベント関係抽出（ERE）フレームワークは、多クラス分類タスクとして扱い、反対称などの異なる関係タイプ間の一貫性を保証しません。もし「嵐」の後に電話回線が「死んだ」場合、明らかに「嵐」が「死んだ」よりも前に起こったことがわかります。現在のイベント関係抽出のフレームワークはこの一貫性を保証せず、制約損失関数を介して強制します（Wang et al.、2020）。本研究では、明示的な制約を適用せずに、各イベントをボックス表現（BERE）として表現することで、一貫性を保証するために基礎となるEREモデルを修正することを提案します。実験から、BEREは制約注入を行った以前のモデルと同等またはそれ以上のF1スコアを示しながら、より強い接続制約の満足度を示すことがわかりました。"}
{"title": "Sample, Translate, Recombine: Leveraging Audio Alignments for Data Augmentation in End-to-end Speech Translation", "url": "https://aclanthology.org/2022.acl-short.27/", "abstract": "End-to-end speech translation relies on data that pair source-language speech inputs with corresponding translations into a target language. Such data are notoriously scarce, making synthetic data augmentation by back-translation or knowledge distillation a necessary ingredient of end-to-end training. In this paper, we present a novel approach to data augmentation that leverages audio alignments, linguistic properties, and translation. First, we augment a transcription by sampling from a suffix memory that stores text and audio data. Second, we translate the augmented transcript. Finally, we recombine concatenated audio segments and the generated translation. Our method delivers consistent improvements of up to 0.9 and 1.1 BLEU points on top of augmentation with knowledge distillation on five language pairs on CoVoST 2 and on two language pairs on Europarl-ST, respectively.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "サンプル、翻訳、再結合：エンドツーエンド音声翻訳におけるデータ拡張のためのオーディオアラインメントの活用", "jabstract": "エンドツーエンドの音声翻訳は、ソース言語の音声入力と対応するターゲット言語への翻訳をペアにしたデータに依存しています。このようなデータは極めて不足しており、バックトランスレーションや知識蒸留による合成データの増幅がエンドツーエンドのトレーニングに必要不可欠です。本論文では、オーディオアラインメント、言語的特性、および翻訳を活用したデータ増幅の新しいアプローチを提案します。まず、テキストとオーディオデータを格納するサフィックスメモリからサンプリングしてトランスクリプトを増幅します。次に、増幅されたトランスクリプトを翻訳します。最後に、連結されたオーディオセグメントと生成された翻訳を再結合します。本手法は、CoVoST 2の5つの言語ペアとEuroparl-STの2つの言語ペアで、知識蒸留による増幅に対して0.9〜1.1 BLEUポイントの一貫した改善をもたらします。"}
{"title": "Predicting Sentence Deletions for Text Simplification Using a Functional Discourse Structure", "url": "https://aclanthology.org/2022.acl-short.28/", "abstract": "Document-level text simplification often deletes some sentences besides performing lexical, grammatical or structural simplification to reduce text complexity. In this work, we focus on sentence deletions for text simplification and use a news genre-specific functional discourse structure, which categorizes sentences based on their contents and their function roles in telling a news story, for predicting sentence deletion. We incorporate sentence categories into a neural net model in two ways for predicting sentence deletions, either as additional features or by jointly predicting sentence deletions and sentence categories. Experimental results using human-annotated data show that incorporating the functional structure improves the recall of sentence deletion prediction by 6.5% and 10.7% respectively using the two methods, and improves the overall F1-score by 3.6% and 4.3% respectively.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "機能的な談話構造を用いたテキスト簡素化のための文削除予測", "jabstract": "文書レベルのテキスト簡素化は、テキストの複雑さを減らすために、語彙、文法、構造の簡素化に加えて、いくつかの文を削除することがよくある。本研究では、テキスト簡素化のための文の削除に焦点を当て、ニュースジャンル固有の機能的なディスコース構造を使用して、ニュースストーリーを伝えるための文の内容と機能的な役割に基づいて文を分類し、文の削除を予測する。文のカテゴリをニューラルネットモデルに2つの方法で組み込み、追加の特徴量としてまたは文の削除と文のカテゴリを共同で予測することで、文の削除を予測する。人間による注釈付きデータを使用した実験結果は、機能的な構造を組み込むことによって、2つの方法でそれぞれ削除予測の再現率が6.5％と10.7％向上し、全体的なF1スコアがそれぞれ3.6％と4.3％向上することを示している。"}
{"title": "Multilingual Pre-training with Language and Task Adaptation for Multilingual Text Style Transfer", "url": "https://aclanthology.org/2022.acl-short.29/", "abstract": "We exploit the pre-trained seq2seq model mBART for multilingual text style transfer. Using machine translated data as well as gold aligned English sentences yields state-of-the-art results in the three target languages we consider. Besides, in view of the general scarcity of parallel data, we propose a modular approach for multilingual formality transfer, which consists of two training strategies that target adaptation to both language and task. Our approach achieves competitive performance without monolingual task-specific parallel data and can be applied to other style transfer tasks as well as to other languages.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "多言語テキストスタイル変換のための言語とタスク適応による多言語プリトレーニング", "jabstract": "私たちは、事前にトレーニングされたseq2seqモデルmBARTを多言語テキストスタイル転送に利用しています。機械翻訳されたデータと英語の正確に整列した文を使用することで、私たちは考慮する3つのターゲット言語で最先端の結果を得ています。また、並列データが一般的に不足していることを考慮して、私たちは多言語フォーマリティ転送のためのモジュラーアプローチを提案しています。このアプローチは、言語とタスクの両方に適応する2つのトレーニング戦略から構成されており、単一言語のタスク固有の並列データなしでも競争力のあるパフォーマンスを発揮し、他のスタイル転送タスクや他の言語にも適用できます。"}
{"title": "When to Use Multi-Task Learning vs Intermediate Fine-Tuning for Pre-Trained Encoder Transfer Learning", "url": "https://aclanthology.org/2022.acl-short.30/", "abstract": "Transfer learning (TL) in natural language processing (NLP) has seen a surge of interest in recent years, as pre-trained models have shown an impressive ability to transfer to novel tasks. Three main strategies have emerged for making use of multiple supervised datasets during fine-tuning: training on an intermediate task before training on the target task (STILTs), using multi-task learning (MTL) to train jointly on a supplementary task and the target task (pairwise MTL), or simply using MTL to train jointly on all available datasets (MTL-ALL). In this work, we compare all three TL methods in a comprehensive analysis on the GLUE dataset suite. We find that there is a simple heuristic for when to use one of these techniques over the other: pairwise MTL is better than STILTs when the target task has fewer instances than the supporting task and vice versa. We show that this holds true in more than 92% of applicable cases on the GLUE dataset and validate this hypothesis with experiments varying dataset size. The simplicity and effectiveness of this heuristic is surprising and warrants additional exploration by the TL community. Furthermore, we find that MTL-ALL is worse than the pairwise methods in almost every case. We hope this study will aid others as they choose between TL methods for NLP tasks.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "事前学習済みエンコーダの転移学習において、マルチタスク学習と中間ファインチューニングの使用タイミングについて。", "jabstract": "自然言語処理（NLP）における転移学習（TL）は、事前学習モデルが新しいタスクに転移する驚異的な能力を示したため、近年注目を集めています。ファインチューニング中に複数の教師ありデータセットを使用するために、3つの主要な戦略が現れました。中間タスクでトレーニングしてからターゲットタスクでトレーニングする（STILTs）、補助タスクとターゲットタスクの両方を同時にトレーニングするマルチタスク学習（MTL）（ペアワイズMTL）、または単にすべての利用可能なデータセットで同時にトレーニングするMTL（MTL-ALL）。本研究では、GLUEデータセットスイートで3つのTL方法を比較的包括的に分析します。ターゲットタスクのインスタンスがサポートタスクよりも少ない場合、ペアワイズMTLがSTILTsよりも優れているという簡単なヒューリスティックがあることがわかりました。逆もまた同様です。GLUEデータセットの適用可能なケースの92％以上で、この仮説がデータセットサイズを変化させた実験で検証されました。このヒューリスティックの単純さと効果は驚くべきものであり、TLコミュニティによる追加の探索を正当化するものです。さらに、MTL-ALLはほとんどの場合でペアワイズ方法よりも劣っていることがわかりました。この研究が、NLPタスクのTL方法を選択する際に他の人々の助けになることを願っています。"}
{"title": "Leveraging Explicit Lexico-logical Alignments in Text-to-SQL Parsing", "url": "https://aclanthology.org/2022.acl-short.31/", "abstract": "Text-to-SQL aims to parse natural language questions into SQL queries, which is valuable in providing an easy interface to access large databases. Previous work has observed that leveraging lexico-logical alignments is very helpful to improve parsing performance. However, current attention-based approaches can only model such alignments at the token level and have unsatisfactory generalization capability. In this paper, we propose a new approach to leveraging explicit lexico-logical alignments. It first identifies possible phrase-level alignments and injects them as additional contexts to guide the parsing procedure. Experimental results on \\textsc{Squall} show that our approach can make better use of such alignments and obtains an absolute improvement of 3.4% compared with the current state-of-the-art.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "テキストからSQLパーシングにおける明示的なレキシコロジカルアラインメントの活用", "jabstract": "Text-to-SQLは、自然言語の質問をSQLクエリに解析することを目的としており、大規模なデータベースにアクセスするための簡単なインターフェースを提供するのに役立ちます。以前の研究では、レキシコロジカルアラインメントを活用することが解析性能の向上に非常に役立つことが観察されています。しかし、現在のアテンションベースのアプローチでは、トークンレベルでしかそのようなアラインメントをモデル化できず、満足できる汎化能力を持っていません。本論文では、明示的なレキシコロジカルアラインメントを活用する新しいアプローチを提案します。まず、可能なフレーズレベルのアラインメントを特定し、解析手順をガイドするための追加の文脈として注入します。 \\textsc{Squall}での実験結果は、当社のアプローチがそのようなアラインメントをより良く活用し、現在の最先端技術と比較して3.4％の絶対的な改善を達成できることを示しています。"}
{"title": "Complex Evolutional Pattern Learning for Temporal Knowledge Graph Reasoning", "url": "https://aclanthology.org/2022.acl-short.32/", "abstract": "A Temporal Knowledge Graph (TKG) is a sequence of KGs corresponding to different timestamps. TKG reasoning aims to predict potential facts in the future given the historical KG sequences. One key of this task is to mine and understand evolutional patterns of facts from these sequences. The evolutional patterns are complex in two aspects, length-diversity and time-variability. Existing models for TKG reasoning focus on modeling fact sequences of a fixed length, which cannot discover complex evolutional patterns that vary in length. Furthermore, these models are all trained offline, which cannot well adapt to the changes of evolutional patterns from then on. Thus, we propose a new model, called Complex Evolutional Network (CEN), which uses a length-aware Convolutional Neural Network (CNN) to handle evolutional patterns of different lengths via an easy-to-difficult curriculum learning strategy. Besides, we propose to learn the model under the online setting so that it can adapt to the changes of evolutional patterns over time. Extensive experiments demonstrate that CEN obtains substantial performance improvement under both the traditional offline and the proposed online settings.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "時間的知識グラフ推論のための複雑な進化的パターン学習", "jabstract": "時間的知識グラフ（TKG）は、異なるタイムスタンプに対応するKGのシーケンスです。TKG推論は、歴史的なKGシーケンスから将来の潜在的な事実を予測することを目的としています。このタスクの1つの鍵は、これらのシーケンスから事実の進化パターンを掘り起こし理解することです。進化パターンは、長さの多様性と時間の変動性の2つの面で複雑です。TKG推論の既存のモデルは、固定長の事実シーケンスをモデル化することに焦点を当てており、長さが異なる複雑な進化パターンを発見することができません。さらに、これらのモデルはすべてオフラインでトレーニングされており、その後の進化パターンの変化に適応することができません。したがって、私たちは、異なる長さの進化パターンを容易から難しいカリキュラム学習戦略を用いて扱う長さに注意した畳み込みニューラルネットワーク（CNN）を使用する新しいモデルであるComplex Evolutional Network（CEN）を提案します。さらに、進化パターンの変化に適応できるように、オンライン設定でモデルを学習することを提案します。広範な実験により、CENは従来のオフライン設定と提案されたオンライン設定の両方で実質的な性能向上を達成することが示されています。"}
{"title": "Mismatch between Multi-turn Dialogue and its Evaluation Metric in Dialogue State Tracking", "url": "https://aclanthology.org/2022.acl-short.33/", "abstract": "Dialogue state tracking (DST) aims to extract essential information from multi-turn dialog situations and take appropriate actions. A belief state, one of the core pieces of information, refers to the subject and its specific content, and appears in the form of domain-slot-value. The trained model predicts “accumulated” belief states in every turn, and joint goal accuracy and slot accuracy are mainly used to evaluate the prediction; however, we specify that the current evaluation metrics have a critical limitation when evaluating belief states accumulated as the dialogue proceeds, especially in the most used MultiWOZ dataset. Additionally, we propose relative slot accuracy to complement existing metrics. Relative slot accuracy does not depend on the number of predefined slots, and allows intuitive evaluation by assigning relative scores according to the turn of each dialog. This study also encourages not solely the reporting of joint goal accuracy, but also various complementary metrics in DST tasks for the sake of a realistic evaluation.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "自然言語処理に関する論文の要約文を日本語に翻訳します。\n\n「多回転対話とその評価メトリックの対応不足についての対話状態追跡」", "jabstract": "対話状態追跡（DST）は、マルチターンの対話状況から必要な情報を抽出し、適切なアクションを取ることを目的としています。情報の中核である信念状態は、主題とその具体的な内容を指し、ドメイン-スロット-値の形式で表されます。訓練されたモデルは、各ターンで「蓄積された」信念状態を予測し、共同目標精度とスロット精度が主に予測の評価に使用されます。しかし、特に最も使用されるMultiWOZデータセットにおいて、現在の評価指標は対話が進行するにつれて蓄積された信念状態を評価する際に重大な制限があることを指摘します。さらに、既存の指標を補完する相対スロット精度を提案します。相対スロット精度は、事前定義されたスロットの数に依存せず、各対話のターンに応じて相対的なスコアを割り当てることで直感的な評価を可能にします。この研究は、DSTタスクにおいて共同目標精度だけでなく、様々な補完的な指標を報告することを奨励し、現実的な評価を目指します。"}
{"title": "LM-BFF-MS: Improving Few-Shot Fine-tuning of Language Models based on Multiple Soft Demonstration Memory", "url": "https://aclanthology.org/2022.acl-short.34/", "abstract": "LM-BFF (CITATION) achieves significant few-shot performance by using auto-generated prompts and adding demonstrations similar to an input example. To improve the approach of LM-BFF, this paper proposes LM-BFF-MS—better few-shot fine-tuning of language models with multiple soft demonstrations by making its further extensions, which include 1) prompts with multiple demonstrations based on automatic generation of multiple label words; and 2) soft demonstration memory which consists of multiple sequences of globally shared word embeddings for a similar context. Experiments conducted on eight NLP tasks show that LM-BFF-MS leads to improvements over LM-BFF on five tasks, particularly achieving 94.0 and 90.4 on SST-2 and MRPC, respectively.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "LM-BFF-MS：多重ソフトデモンストレーションメモリに基づく言語モデルのフューションファインチューニングの少数ショットの改善", "jabstract": "LM-BFF（引用）は、自動生成されたプロンプトを使用し、入力例に類似したデモンストレーションを追加することにより、重要なフューショットのパフォーマンスを達成します。LM-BFFのアプローチを改善するために、本論文では、複数のソフトデモンストレーションを使用した言語モデルのより良いフューショットファインチューニングであるLM-BFF-MSを提案します。これには、1）複数のラベル単語の自動生成に基づく複数のデモンストレーションを含むプロンプト、および2）類似した文脈のための複数のグローバル共有単語埋め込みの複数のシーケンスから構成されるソフトデモンストレーションメモリが含まれます。8つのNLPタスクで実験を行った結果、LM-BFF-MSは、特にSST-2とMRPCでそれぞれ94.0と90.4を達成し、LM-BFFよりも5つのタスクで改善をもたらします。"}
{"title": "Towards Fair Evaluation of Dialogue State Tracking by Flexible Incorporation of Turn-level Performances", "url": "https://aclanthology.org/2022.acl-short.35/", "abstract": "Dialogue State Tracking (DST) is primarily evaluated using Joint Goal Accuracy (JGA) defined as the fraction of turns where the ground-truth dialogue state exactly matches the prediction. Generally in DST, the dialogue state or belief state for a given turn contain all the intents shown by the user till that turn. Due to this cumulative nature of the belief state, it is difficult to get a correct prediction once a misprediction has occurred. Thus, although being a useful metric, it can be harsh at times and underestimate the true potential of a DST model. Moreover, an improvement in JGA can sometimes decrease the performance of turn-level or non-cumulative belief state prediction due to inconsistency in annotations. So, using JGA as the only metric for model selection may not be ideal for all scenarios. In this work, we discuss various evaluation metrics used for DST along with their shortcomings. To address the existing issues, we propose a new evaluation metric named Flexible Goal Accuracy (FGA). FGA is a generalized version of JGA. But unlike JGA, it tries to give penalized rewards to mispredictions that are locally correct i.e. the root cause of the error is an earlier turn. By doing so, FGA considers the performance of both cumulative and turn-level prediction flexibly and provides a better insight than the existing metrics. We also show that FGA is a better discriminator of DST model performance.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "「ターンレベルのパフォーマンスを柔軟に組み込むことによる対話状態追跡の公正な評価に向けて」の要旨です。", "jabstract": "Dialogue State Tracking（DST）は、正確に予測された対話状態と正解の対話状態が完全に一致するターンの割合として定義されるJoint Goal Accuracy（JGA）を使用して主に評価されます。一般的に、DSTでは、特定のターンの対話状態または信念状態には、そのターンまでにユーザーによって示されたすべての意図が含まれます。信念状態のこの累積的な性質のため、誤った予測が発生した場合に正しい予測を得ることは困難です。したがって、有用なメトリックであるにもかかわらず、時には厳しすぎる場合があり、DSTモデルの真のポテンシャルを過小評価する可能性があります。さらに、JGAの改善は、注釈の不一致により、ターンレベルまたは非累積的な信念状態予測のパフォーマンスを低下させる場合があります。したがって、モデル選択の唯一のメトリックとしてJGAを使用することは、すべてのシナリオにとって理想的ではありません。この論文では、DSTに使用されるさまざまな評価メトリックとその欠点について説明します。既存の問題に対処するために、Flexible Goal Accuracy（FGA）という新しい評価メトリックを提案します。FGAは、JGAの一般化バージョンです。しかし、JGAとは異なり、ローカルに正しい誤った予測に罰則的な報酬を与えようとします。つまり、エラーの原因が以前のターンである場合です。これにより、FGAは累積的およびターンレベルの予測の両方のパフォーマンスを柔軟に考慮し、既存のメトリックよりも優れた洞察を提供します。また、FGAがDSTモデルのパフォーマンスのより優れた識別子であることを示します。"}
{"title": "Exploiting Language Model Prompts Using Similarity Measures: A Case Study on the Word-in-Context Task", "url": "https://aclanthology.org/2022.acl-short.36/", "abstract": "As a recent development in few-shot learning, prompt-based techniques have demonstrated promising potential in a variety of natural language processing tasks. However, despite proving competitive on most tasks in the GLUE and SuperGLUE benchmarks, existing prompt-based techniques fail on the semantic distinction task of the Word-in-Context (WiC) dataset. Specifically, none of the existing few-shot approaches (including the in-context learning of GPT-3) can attain a performance that is meaningfully different from the random baseline.Trying to fill this gap, we propose a new prompting technique, based on similarity metrics, which boosts few-shot performance to the level of fully supervised methods. Our simple adaptation shows that the failure of existing prompt-based techniques in semantic distinction is due to their improper configuration, rather than lack of relevant knowledge in the representations. We also show that this approach can be effectively extended to other downstream tasks for which a single prompt is sufficient.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "類似性尺度を利用した言語モデルプロンプトの活用：文脈内語タスクにおける事例研究", "jabstract": "最近のfew-shot learningの発展として、prompt-based技術は、様々な自然言語処理タスクで有望な可能性を示しています。しかし、GLUEおよびSuperGLUEベンチマークのほとんどのタスクで競争力を証明しているにもかかわらず、既存のprompt-based技術は、Word-in-Context（WiC）データセットの意味的区別タスクで失敗しています。具体的には、GPT-3のin-context学習を含む既存のfew-shotアプローチのいずれも、ランダムベースラインと意味的に異なるパフォーマンスを達成することができません。このギャップを埋めるために、類似性メトリックに基づく新しいprompting技術を提案し、few-shotパフォーマンスを完全に監視された方法のレベルまで向上させます。私たちのシンプルな適応は、既存のprompt-based技術の意味的区別における失敗は、表現に関連する知識の欠如ではなく、不適切な構成に起因することを示しています。また、このアプローチが、単一のプロンプトが十分な他のダウンストリームタスクに効果的に拡張できることも示しています。"}
{"title": "Hierarchical Curriculum Learning for AMR Parsing", "url": "https://aclanthology.org/2022.acl-short.37/", "abstract": "Abstract Meaning Representation (AMR) parsing aims to translate sentences to semantic representation with a hierarchical structure, and is recently empowered by pretrained sequence-to-sequence models. However, there exists a gap between their flat training objective (i.e., equally treats all output tokens) and the hierarchical AMR structure, which limits the model generalization. To bridge this gap, we propose a Hierarchical Curriculum Learning (HCL) framework with Structure-level (SC) and Instance-level Curricula (IC). SC switches progressively from core to detail AMR semantic elements while IC transits from structure-simple to -complex AMR instances during training. Through these two warming-up processes, HCL reduces the difficulty of learning complex structures, thus the flat model can better adapt to the AMR hierarchy. Extensive experiments on AMR2.0, AMR3.0, structure-complex and out-of-distribution situations verify the effectiveness of HCL.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "AMRパーシングのための階層カリキュラム学習", "jabstract": "Abstract Meaning Representation（AMR）パーシングは、階層構造を持つ意味表現に文を翻訳することを目的としており、最近では事前学習されたシーケンス・トゥ・シーケンス・モデルによって強化されています。しかし、彼らのフラットなトレーニング目標（すべての出力トークンを同等に扱う）と階層的なAMR構造との間にはギャップがあり、モデルの汎化を制限しています。このギャップを埋めるために、私たちは構造レベル（SC）とインスタンスレベルのカリキュラム学習（IC）を備えた階層的カリキュラム学習（HCL）フレームワークを提案します。SCは、トレーニング中にコアから詳細なAMR意味要素に徐々に切り替わり、ICはトレーニング中に構造が単純なAMRインスタンスから複雑なものに移行します。これらの2つのウォーミングアッププロセスにより、HCLは複雑な構造を学習する難しさを減らし、フラットモデルがAMR階層に適応することができるようになります。AMR2.0、AMR3.0、構造複雑性、および分布外状況に対する広範な実験により、HCLの効果が検証されました。"}
{"title": "PARE: A Simple and Strong Baseline for Monolingual and Multilingual Distantly Supervised Relation Extraction", "url": "https://aclanthology.org/2022.acl-short.38/", "abstract": "Neural models for distantly supervised relation extraction (DS-RE) encode each sentence in an entity-pair bag separately. These are then aggregated for bag-level relation prediction. Since, at encoding time, these approaches do not allow information to flow from other sentences in the bag, we believe that they do not utilize the available bag data to the fullest. In response, we explore a simple baseline approach (PARE) in which all sentences of a bag are concatenated into a passage of sentences, and encoded jointly using BERT. The contextual embeddings of tokens are aggregated using attention with the candidate relation as query – this summary of whole passage predicts the candidate relation. We find that our simple baseline solution outperforms existing state-of-the-art DS-RE models in both monolingual and multilingual DS-RE datasets.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "PARE：単一言語および多言語遠隔監視関係抽出のシンプルで強力なベースライン", "jabstract": "「遠隔監視関係抽出（DS-RE）のためのニューラルモデルは、エンティティペアバッグ内の各文を個別にエンコードします。それらはバッグレベルの関係予測のために集約されます。エンコード時に、これらのアプローチはバッグ内の他の文から情報が流れないため、利用可能なバッグデータを最大限に活用していないと考えています。そのため、私たちは、バッグ内のすべての文を連結して文のパッセージにし、BERTを使用して共同でエンコードする単純なベースラインアプローチ（PARE）を探求しました。トークンの文脈埋め込みは、候補関係をクエリとして使用して注意を集約し、このパッセージ全体の要約が候補関係を予測します。私たちは、私たちの単純なベースラインソリューションが、単一言語および多言語のDS-REデータセットの両方で既存の最先端のDS-REモデルを上回ることを発見しました。」"}
{"title": "To Find Waldo You Need Contextual Cues: Debiasing Who’s Waldo", "url": "https://aclanthology.org/2022.acl-short.39/", "abstract": "We present a debiased dataset for the Person-centric Visual Grounding (PCVG) task first proposed by Cui et al. (2021) in the Who’s Waldo dataset. Given an image and a caption, PCVG requires pairing up a person’s name mentioned in a caption with a bounding box that points to the person in the image. We find that the original Who’s Waldo dataset compiled for this task contains a large number of biased samples that are solvable simply by heuristic methods; for instance, in many cases the first name in the sentence corresponds to the largest bounding box, or the sequence of names in the sentence corresponds to an exact left-to-right order in the image. Naturally, models trained on these biased data lead to over-estimation of performance on the benchmark. To enforce models being correct for the correct reasons, we design automated tools to filter and debias the original dataset by ruling out all examples of insufficient context, such as those with no verb or with a long chain of conjunct names in their captions. Our experiments show that our new sub-sampled dataset contains less bias with much lowered heuristic performances and widened gaps between heuristic and supervised methods. We also demonstrate the same benchmark model trained on our debiased training set outperforms that trained on the original biased (and larger) training set on our debiased test set. We argue our debiased dataset offers the PCVG task a more practical baseline for reliable benchmarking and future improvements.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "自然言語処理に関する論文の要約文を日本語に翻訳してください。\n\nウォルドを見つけるには文脈的手がかりが必要です：ウォルドを偏見から解放する", "jabstract": "私たちは、Cuiら（2021）によって最初に提案されたWho's WaldoデータセットのPerson-centric Visual Grounding（PCVG）タスクのための偏りのないデータセットを提供します。画像とキャプションが与えられた場合、PCVGは、キャプションで言及された人物の名前を、画像内のその人物を指す境界ボックスにペアリングすることを必要とします。私たちは、このタスクに編成された元のWho's Waldoデータセットには、ヒューリスティックな方法で解決できるバイアスのあるサンプルが多数含まれていることを発見しました。例えば、文の最初の名前が最大の境界ボックスに対応する場合や、文の名前の順序が画像内の正確な左から右の順序に対応する場合が多いです。自然に、これらのバイアスのあるデータで訓練されたモデルは、ベンチマークでのパフォーマンスの過大評価につながります。正しい理由で正しいモデルを強制するために、私たちは、動詞がない、またはキャプションに長い連鎖の接続名があるなど、不十分な文脈のすべての例を除外するための自動化されたツールを設計し、元のデータセットをデバイスしました。私たちの実験は、私たちの新しいサブサンプルされたデータセットが、ヒューリスティックなパフォーマンスが低下し、ヒューリスティックと監視された方法の間のギャップが広がった、より少ないバイアスを含むことを示しています。また、私たちは、私たちのデバイスされたトレーニングセットでトレーニングされた同じベンチマークモデルが、デバイスされたテストセットでトレーニングされた元のバイアスのある（そして大きな）トレーニングセットでトレーニングされたモデルよりも優れていることを示しています。私たちは、私たちのデバイスされたデータセットが、PCVGタスクに信頼できるベンチマークと将来の改善のためのより実用的なベースラインを提供すると主張しています。"}
{"title": "Translate-Train Embracing Translationese Artifacts", "url": "https://aclanthology.org/2022.acl-short.40/", "abstract": "Translate-train is a general training approach to multilingual tasks. The key idea is to use the translator of the target language to generate training data to mitigate the gap between the source and target languages. However, its performance is often hampered by the artifacts in the translated texts (translationese). We discover that such artifacts have common patterns in different languages and can be modeled by deep learning, and subsequently propose an approach to conduct translate-train using Translationese Embracing the effect of Artifacts (TEA). TEA learns to mitigate such effect on the training data of a source language (whose original and translationese are both available), and applies the learned module to facilitate the inference on the target language. Extensive experiments on the multilingual QA dataset TyDiQA demonstrate that TEA outperforms strong baselines.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "「Translate-Train Embracing Translationese Artifacts」という論文の要約文を日本語に翻訳してください。", "jabstract": "Translate-trainは、多言語タスクの一般的なトレーニングアプローチです。その主要なアイデアは、ターゲット言語の翻訳者を使用してトレーニングデータを生成し、ソース言語とターゲット言語のギャップを緩和することです。しかし、翻訳されたテキストのアーティファクト（翻訳語）によって、そのパフォーマンスがしばしば妨げられます。我々は、このようなアーティファクトが異なる言語で共通のパターンを持ち、深層学習によってモデル化できることを発見し、その後、Translationese Embracing the effect of Artifacts（TEA）を使用してtranslate-trainを実行するアプローチを提案します。TEAは、ソース言語のトレーニングデータ（元のテキストと翻訳語の両方が利用可能な）におけるそのような影響を緩和することを学習し、学習されたモジュールをターゲット言語の推論を促進するために適用します。TyDiQAという多言語QAデータセットでの広範な実験は、TEAが強力なベースラインを上回ることを示しています。"}
{"title": "C-MORE: Pretraining to Answer Open-Domain Questions by Consulting Millions of References", "url": "https://aclanthology.org/2022.acl-short.41/", "abstract": "We consider the problem of pretraining a two-stage open-domain question answering (QA) system (retriever + reader) with strong transfer capabilities. The key challenge is how to construct a large amount of high-quality question-answer-context triplets without task-specific annotations. Specifically, the triplets should align well with downstream tasks by: (i) covering a wide range of domains (for open-domain applications), (ii) linking a question to its semantically relevant context with supporting evidence (for training the retriever), and (iii) identifying the correct answer in the context (for training the reader). Previous pretraining approaches generally fall short of one or more of these requirements. In this work, we automatically construct a large-scale corpus that meets all three criteria by consulting millions of references cited within Wikipedia. The well-aligned pretraining signals benefit both the retriever and the reader significantly. Our pretrained retriever leads to 2%-10% absolute gains in top-20 accuracy. And with our pretrained reader, the entire system improves by up to 4% in exact match.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "C-MORE：数百万の参照を参照してオープンドメインの質問に答えるための事前トレーニング", "jabstract": "私たちは、強力な転移能力を持つ2段階のオープンドメインの質問応答（QA）システム（リトリーバー+リーダー）の事前トレーニングの問題を考慮しています。主な課題は、タスク固有の注釈なしで高品質な質問-回答-文脈の三つ組を大量に構築する方法です。具体的には、三つ組は、（i）広範なドメインをカバーする（オープンドメインアプリケーションのため）、（ii）質問をその意味的に関連する文脈と証拠を提供してリトリーバーをトレーニングする（iii）文脈内で正しい回答を特定する（リーダーをトレーニングするため）ことによって、ダウンストリームタスクとよく一致する必要があります。以前の事前トレーニングアプローチは、これらの要件のいずれかを満たすことができませんでした。この研究では、Wikipediaに引用された数百万の参照を参照して、これらの3つの基準をすべて満たす大規模なコーパスを自動的に構築します。よく整列された事前トレーニング信号は、リトリーバーとリーダーの両方に大きな利益をもたらします。私たちの事前トレーニングされたリトリーバーは、トップ20の精度で2％〜10％の絶対的な利益をもたらします。そして、私たちの事前トレーニングされたリーダーで、システム全体が完全一致で最大4％改善されます。"}
{"title": "k-Rater Reliability: The Correct Unit of Reliability for Aggregated Human Annotations", "url": "https://aclanthology.org/2022.acl-short.42/", "abstract": "Since the inception of crowdsourcing, aggregation has been a common strategy for dealing with unreliable data. Aggregate ratings are more reliable than individual ones. However, many Natural Language Processing (NLP) applications that rely on aggregate ratings only report the reliability of individual ratings, which is the incorrect unit of analysis. In these instances, the data reliability is under-reported, and a proposed k-rater reliability (kRR) should be used as the correct data reliability for aggregated datasets. It is a multi-rater generalization of inter-rater reliability (IRR). We conducted two replications of the WordSim-353 benchmark, and present empirical, analytical, and bootstrap-based methods for computing kRR on WordSim-353. These methods produce very similar results. We hope this discussion will nudge researchers to report kRR in addition to IRR.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "k-レータ信頼性：集約された人間の注釈の正しい信頼性の単位", "jabstract": "クラウドソーシングの発展以来、信頼性の低いデータを扱うための一般的な戦略として、集計が採用されてきました。集計された評価は個別の評価よりも信頼性が高いです。しかし、集計された評価に依存する多くの自然言語処理（NLP）アプリケーションは、個別の評価の信頼性しか報告しておらず、これは誤った分析単位です。これらの場合、データの信頼性が過小報告され、集計されたデータセットの正しいデータ信頼性として提案されるk-rater reliability（kRR）を使用する必要があります。これは、インターレータ信頼性（IRR）の多重レーター一般化です。私たちはWordSim-353ベンチマークの2つの複製を実施し、WordSim-353でkRRを計算するための経験的、分析的、ブートストラップベースの方法を提示します。これらの方法は非常に似た結果を生み出します。この議論が研究者たちにIRRに加えてkRRを報告するよう促すことを願っています。"}
{"title": "An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers", "url": "https://aclanthology.org/2022.acl-short.43/", "abstract": "We introduce FLOTA (Few Longest Token Approximation), a simple yet effective method to improve the tokenization of pretrained language models (PLMs). FLOTA uses the vocabulary of a standard tokenizer but tries to preserve the morphological structure of words during tokenization. We evaluate FLOTA on morphological gold segmentations as well as a text classification task, using BERT, GPT-2, and XLNet as example PLMs. FLOTA leads to performance gains, makes inference more efficient, and enhances the robustness of PLMs with respect to whitespace noise.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "事前学習された言語モデルのトークナイザーの望ましくない特性を軽減するための恥ずかしく単純な方法", "jabstract": "私たちは、事前学習言語モデル（PLM）のトークン化を改善するためのシンプルで効果的な方法であるFLOTA（Few Longest Token Approximation）を紹介する。FLOTAは、標準的なトークナイザーの語彙を使用しながら、単語の形態構造をトークン化の過程で保持しようとする。私たちは、BERT、GPT-2、およびXLNetを例として、形態論的なゴールドセグメンテーションとテキスト分類タスクでFLOTAを評価する。FLOTAは、性能向上、推論の効率化、およびPLMのホワイトスペースノイズに対する堅牢性の向上につながる。"}
{"title": "SCD: Self-Contrastive Decorrelation of Sentence Embeddings", "url": "https://aclanthology.org/2022.acl-short.44/", "abstract": "In this paper, we propose Self-Contrastive Decorrelation (SCD), a self-supervised approach. Given an input sentence, it optimizes a joint self-contrastive and decorrelation objective. Learning a representation is facilitated by leveraging the contrast arising from the instantiation of standard dropout at different rates. The proposed method is conceptually simple yet empirically powerful. It achieves comparable results with state-of-the-art methods on multiple benchmarks without using contrastive pairs. This study opens up avenues for efficient self-supervised learning methods that are more robust than current contrastive methods.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "SCD：文の埋め込みの自己対照的な相関解除", "jabstract": "本論文では、自己対照的な装飾 (SCD) という自己教師ありアプローチを提案します。入力文が与えられると、共同自己対照的および装飾目的を最適化します。表現の学習は、異なるレートで標準的なドロップアウトの具現化から生じる対比を活用することによって容易になります。提案された方法は概念的にはシンプルですが、実証的には強力です。対照的なペアを使用せずに、複数のベンチマークで最先端の方法と比較可能な結果を達成します。この研究は、現在の対照的な方法よりも堅牢な効率的な自己教師あり学習方法の可能性を開拓します。"}
{"title": "Problems with Cosine as a Measure of Embedding Similarity for High Frequency Words", "url": "https://aclanthology.org/2022.acl-short.45/", "abstract": "Cosine similarity of contextual embeddings is used in many NLP tasks (e.g., QA, IR, MT) and metrics (e.g., BERTScore). Here, we uncover systematic ways in which word similarities estimated by cosine over BERT embeddings are understated and trace this effect to training data frequency. We find that relative to human judgements, cosine similarity underestimates the similarity of frequent words with other instances of the same word or other words across contexts, even after controlling for polysemy and other factors. We conjecture that this underestimation of similarity for high frequency words is due to differences in the representational geometry of high and low frequency words and provide a formal argument for the two-dimensional case.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "高頻度単語の埋め込み類似度の測定におけるコサインの問題点", "jabstract": "文脈埋め込みのコサイン類似度は、多くのNLPタスク（例：QA、IR、MT）およびメトリック（例：BERTScore）で使用されています。ここでは、BERT埋め込みのコサインによって推定される単語の類似性が過小評価されるシステマティックな方法を明らかにし、この効果をトレーニングデータの頻度に追跡します。私たちは、ポリセミーなどの要因を制御した後でも、コサイン類似度が、頻出語と同じ単語の他のインスタンスや他の文脈の単語との類似性を過小評価していることを発見しました。私たちは、高頻度語の類似性の過小評価が、高頻度語と低頻度語の表現幾何学の違いに起因すると推測し、2次元の場合について形式的な議論を提供します。"}
{"title": "Revisiting the Compositional Generalization Abilities of Neural Sequence Models", "url": "https://aclanthology.org/2022.acl-short.46/", "abstract": "Compositional generalization is a fundamental trait in humans, allowing us to effortlessly combine known phrases to form novel sentences. Recent works have claimed that standard seq-to-seq models severely lack the ability to compositionally generalize. In this paper, we focus on one-shot primitive generalization as introduced by the popular SCAN benchmark. We demonstrate that modifying the training distribution in simple and intuitive ways enables standard seq-to-seq models to achieve near-perfect generalization performance, thereby showing that their compositional generalization abilities were previously underestimated. We perform detailed empirical analysis of this phenomenon. Our results indicate that the generalization performance of models is highly sensitive to the characteristics of the training data which should be carefully considered while designing such benchmarks in future.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "「ニューラルシーケンスモデルの構成的一般化能力を再考する」の論文の要約文を日本語に翻訳してください。", "jabstract": "合成的な一般化は、人間にとって基本的な特性であり、既知のフレーズを容易に組み合わせて新しい文を形成することができます。最近の研究では、標準的なseq-to-seqモデルは合成的な一般化の能力に深刻な欠陥があると主張しています。本論文では、人気のあるSCANベンチマークで導入された一発プリミティブ一般化に焦点を当てます。私たちは、トレーニング分布を簡単で直感的な方法で変更することで、標準的なseq-to-seqモデルがほぼ完璧な一般化性能を発揮できることを示し、彼らの合成的な一般化能力が以前に過小評価されていたことを示します。この現象について詳細な実証分析を行います。私たちの結果は、モデルの一般化性能がトレーニングデータの特性に非常に敏感であり、将来的にこのようなベンチマークを設計する際には注意深く考慮する必要があることを示しています。"}
{"title": "A Copy-Augmented Generative Model for Open-Domain Question Answering", "url": "https://aclanthology.org/2022.acl-short.47/", "abstract": "Open-domain question answering is a challenging task with a wide variety of practical applications. Existing modern approaches mostly follow a standard two-stage paradigm: retriever then reader. In this article, we focus on improving the effectiveness of the reader module and propose a novel copy-augmented generative approach that integrates the merits of both extractive and generative readers. In particular, our model is built upon the powerful generative model FiD (CITATION). We enhance the original generative reader by incorporating a pointer network to encourage the model to directly copy words from the retrieved passages. We conduct experiments on the two benchmark datasets, Natural Questions and TriviaQA, and the empirical results demonstrate the performance gains of our proposed approach.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "オープンドメインの質問応答のためのコピー拡張生成モデル", "jabstract": "オープンドメインの質問応答は、実用的なアプリケーションが多岐にわたる難しいタスクです。現代のアプローチは、主に検索器とリーダーの2段階のパラダイムに従っています。本論文では、リーダーモジュールの効果を改善することに焦点を当て、抽出型リーダーと生成型リーダーの両方の利点を統合した新しいコピー拡張生成アプローチを提案します。特に、私たちのモデルは、強力な生成モデルFiDに基づいて構築されています。私たちは、ポインターネットワークを組み込むことで、モデルが検索されたパッセージから直接単語をコピーするように促すことで、元の生成リーダーを強化します。私たちは、2つのベンチマークデータセット、Natural QuestionsとTriviaQAで実験を行い、提案されたアプローチの性能向上を実証する経験的な結果を示します。"}
{"title": "Augmenting Document Representations for Dense Retrieval with Interpolation and Perturbation", "url": "https://aclanthology.org/2022.acl-short.48/", "abstract": "Dense retrieval models, which aim at retrieving the most relevant document for an input query on a dense representation space, have gained considerable attention for their remarkable success. Yet, dense models require a vast amount of labeled training data for notable performance, whereas it is often challenging to acquire query-document pairs annotated by humans. To tackle this problem, we propose a simple but effective Document Augmentation for dense Retrieval (DAR) framework, which augments the representations of documents with their interpolation and perturbation. We validate the performance of DAR on retrieval tasks with two benchmark datasets, showing that the proposed DAR significantly outperforms relevant baselines on the dense retrieval of both the labeled and unlabeled documents.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "「補間と摂動を用いた密な検索のための文書表現の拡張」に関する論文の要約文を日本語に翻訳してください。", "jabstract": "密集検索モデルは、密集表現空間上で入力クエリに最も関連性の高い文書を検索することを目的としており、その優れた成功により注目を集めています。しかし、密集モデルは、著しいパフォーマンスを発揮するためには大量のラベル付きトレーニングデータが必要であり、人間によって注釈付けされたクエリ-ドキュメントのペアを取得することはしばしば困難です。この問題に対処するために、私たちは、ドキュメントの表現をその内挿と摂動によって拡張する単純で効果的な密集検索のためのドキュメント拡張（DAR）フレームワークを提案します。私たちは、2つのベンチマークデータセットで検索タスクのDARのパフォーマンスを検証し、提案されたDARがラベル付きおよびラベルなしの両方の文書の密集検索において関連するベースラインを大幅に上回ることを示しています。"}
{"title": "WLASL-LEX: a Dataset for Recognising Phonological Properties in American Sign Language", "url": "https://aclanthology.org/2022.acl-short.49/", "abstract": "Signed Language Processing (SLP) concerns the automated processing of signed languages, the main means of communication of Deaf and hearing impaired individuals. SLP features many different tasks, ranging from sign recognition to translation and production of signed speech, but has been overlooked by the NLP community thus far.In this paper, we bring to attention the task of modelling the phonology of sign languages. We leverage existing resources to construct a large-scale dataset of American Sign Language signs annotated with six different phonological properties. We then conduct an extensive empirical study to investigate whether data-driven end-to-end and feature-based approaches can be optimised to automatically recognise these properties. We find that, despite the inherent challenges of the task, graph-based neural networks that operate over skeleton features extracted from raw videos are able to succeed at the task to a varying degree. Most importantly, we show that this performance pertains even on signs unobserved during training.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "WLASL-LEX：アメリカ手話の音韻的特性を認識するためのデータセット", "jabstract": "署名言語処理（SLP）は、聴覚障害者や聴覚障害者の主要なコミュニケーション手段である署名言語の自動処理に関するものである。SLPには、署名認識から署名音声の翻訳や生成まで、さまざまなタスクがありますが、これまでNLPコミュニティに見落とされてきました。本論文では、署名言語の音韻モデリングのタスクに注目します。既存のリソースを活用して、6つの異なる音韻的特性で注釈付けされたアメリカンサインランゲージの署名の大規模なデータセットを構築しました。その後、データ駆動型のエンドツーエンドアプローチと特徴ベースのアプローチを最適化して、これらの特性を自動的に認識できるかどうかを調査するために、広範な実証研究を行いました。私たちは、タスクの固有の課題にもかかわらず、生のビデオから抽出されたスケルトン特徴を操作するグラフベースのニューラルネットワークが、さまざまな程度でタスクに成功することができることを発見しました。最も重要なことは、トレーニング中に観察されなかった署名でも、このパフォーマンスが維持されることを示したことです。"}
{"title": "Investigating person-specific errors in chat-oriented dialogue systems", "url": "https://aclanthology.org/2022.acl-short.50/", "abstract": "Creating chatbots to behave like real people is important in terms of believability. Errors in general chatbots and chatbots that follow a rough persona have been studied, but those in chatbots that behave like real people have not been thoroughly investigated. We collected a large amount of user interactions of a generation-based chatbot trained from large-scale dialogue data of a specific character, i.e., target person, and analyzed errors related to that person. We found that person-specific errors can be divided into two types: errors in attributes and those in relations, each of which can be divided into two levels: self and other. The correspondence with an existing taxonomy of errors was also investigated, and person-specific errors that should be addressed in the future were clarified.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "チャット志向の対話システムにおける個人特有のエラーの調査", "jabstract": "現実的な人物のように振る舞うチャットボットを作成することは、信憑性の観点から重要である。一般的なチャットボットや荒っぽいパーソナリティを持つチャットボットのエラーについては研究されてきたが、現実的な人物のように振る舞うチャットボットのエラーについては十分に調査されていない。我々は、特定のキャラクター、すなわちターゲットの人物の大規模な対話データからトレーニングされた世代ベースのチャットボットのユーザーインタラクションの大量のデータを収集し、その人物に関連するエラーを分析した。我々は、人物固有のエラーを属性のエラーと関係のエラーの2つに分類できることを発見し、それぞれが自己と他者の2つのレベルに分けられることができることも明らかにした。既存のエラーのタクソノミーとの対応も調査し、将来的に対処すべき人物固有のエラーが明確にされた。"}
{"title": "Direct parsing to sentiment graphs", "url": "https://aclanthology.org/2022.acl-short.51/", "abstract": "This paper demonstrates how a graph-based semantic parser can be applied to the task of structured sentiment analysis, directly predicting sentiment graphs from text. We advance the state of the art on 4 out of 5 standard benchmark sets. We release the source code, models and predictions.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "自然言語処理に関する論文の要約文を日本語に翻訳してください。\n\n感情グラフへの直接解析", "jabstract": "この論文では、グラフベースの意味解析器が構造化された感情分析のタスクに適用され、テキストから直接感情グラフを予測する方法を示しています。我々は、5つの標準ベンチマークセットのうち4つで最新技術を進めました。我々はソースコード、モデル、予測を公開します。"}
{"title": "XDBERT: Distilling Visual Information to BERT from Cross-Modal Systems to Improve Language Understanding", "url": "https://aclanthology.org/2022.acl-short.52/", "abstract": "Transformer-based models are widely used in natural language understanding (NLU) tasks, and multimodal transformers have been effective in visual-language tasks. This study explores distilling visual information from pretrained multimodal transformers to pretrained language encoders. Our framework is inspired by cross-modal encoders’ success in visual-language tasks while we alter the learning objective to cater to the language-heavy characteristics of NLU. After training with a small number of extra adapting steps and finetuned, the proposed XDBERT (cross-modal distilled BERT) outperforms pretrained-BERT in general language understanding evaluation (GLUE), situations with adversarial generations (SWAG) benchmarks, and readability benchmarks. We analyze the performance of XDBERT on GLUE to show that the improvement is likely visually grounded.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "XDBERT：言語理解を向上させるために、クロスモーダルシステムからBERTに視覚情報を蒸留する", "jabstract": "トランスフォーマーベースのモデルは、自然言語理解（NLU）タスクで広く使用されており、マルチモーダルトランスフォーマーは視覚言語タスクで効果的であることが示されています。本研究では、事前学習されたマルチモーダルトランスフォーマーから視覚情報を言語エンコーダーに蒸留することを探求しています。我々のフレームワークは、クロスモーダルエンコーダーが視覚言語タスクで成功していることに着想を得ていますが、学習目標をNLUの言語重視の特性に合わせて変更しています。少数の追加適応ステップでトレーニングし、微調整することで、提案されたXDBERT（クロスモーダル蒸留BERT）は、一般言語理解評価（GLUE）、敵対的生成（SWAG）ベンチマーク、読みやすさベンチマークで、事前学習されたBERTを上回る性能を発揮します。我々は、XDBERTのGLUEでのパフォーマンスを分析し、改善が視覚的に根付いている可能性があることを示します。"}
{"title": "As Little as Possible, as Much as Necessary: Detecting Over- and Undertranslations with Contrastive Conditioning", "url": "https://aclanthology.org/2022.acl-short.53/", "abstract": "Omission and addition of content is a typical issue in neural machine translation. We propose a method for detecting such phenomena with off-the-shelf translation models. Using contrastive conditioning, we compare the likelihood of a full sequence under a translation model to the likelihood of its parts, given the corresponding source or target sequence. This allows to pinpoint superfluous words in the translation and untranslated words in the source even in the absence of a reference translation. The accuracy of our method is comparable to a supervised method that requires a custom quality estimation model.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "できるだけ少なく、必要最小限に: 対比的な調整を用いた過剰・不足翻訳の検出", "jabstract": "内容の省略や追加は、ニューラル機械翻訳における典型的な問題です。本研究では、市販の翻訳モデルを用いて、このような現象を検出する方法を提案します。対照的な条件付けを用いて、翻訳モデルにおける完全なシーケンスの尤度と、対応するソースまたはターゲットシーケンスの部分の尤度を比較することで、翻訳に余分な単語やソースに翻訳されていない単語を特定することができます。これにより、参照翻訳がなくても、翻訳における余分な単語やソースにおける未翻訳の単語を特定することができます。本方法の精度は、カスタム品質評価モデルを必要とする教師あり方法と比較して同等です。"}
{"title": "How Distributed are Distributed Representations? An Observation on the Locality of Syntactic Information in Verb Agreement Tasks", "url": "https://aclanthology.org/2022.acl-short.54/", "abstract": "This work addresses the question of the localization of syntactic information encoded in the transformers representations. We tackle this question from two perspectives, considering the object-past participle agreement in French, by identifying, first, in which part of the sentence and, second, in which part of the representation the syntactic information is encoded. The results of our experiments, using probing, causal analysis and feature selection method, show that syntactic information is encoded locally in a way consistent with the French grammar.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "分散表現はどの程度分散しているのか？動詞の一致タスクにおける構文情報の局所性に関する観察。", "jabstract": "この研究は、トランスフォーマー表現にエンコードされた構文情報のローカリゼーションの問題に取り組んでいます。私たちは、フランス語における目的語-過去分詞の一致を考慮し、まず文のどの部分に、そして次に表現のどの部分に構文情報がエンコードされているかを特定することで、この問題に取り組んでいます。プロービング、因果分析、特徴選択法を使用した実験の結果、構文情報がフランス語の文法に一致する方法でローカルにエンコードされていることが示されました。"}
{"title": "Machine Translation for Livonian: Catering to 20 Speakers", "url": "https://aclanthology.org/2022.acl-short.55/", "abstract": "Livonian is one of the most endangered languages in Europe with just a tiny handful of speakers and virtually no publicly available corpora. In this paper we tackle the task of developing neural machine translation (NMT) between Livonian and English, with a two-fold aim: on one hand, preserving the language and on the other – enabling access to Livonian folklore, lifestories and other textual intangible heritage as well as making it easier to create further parallel corpora. We rely on Livonian’s linguistic similarity to Estonian and Latvian and collect parallel and monolingual data for the four languages for translation experiments. We combine different low-resource NMT techniques like zero-shot translation, cross-lingual transfer and synthetic data creation to reach the highest possible translation quality as well as to find which base languages are empirically more helpful for transfer to Livonian. The resulting NMT systems and the collected monolingual and parallel data, including a manually translated and verified translation benchmark, are publicly released via OPUS and Huggingface repositories.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "リヴォニア語の機械翻訳：20人の話者に対応する\n\nAbstract:\nThis paper presents the development of a machine translation system for Livonian, a critically endangered Finno-Ugric language spoken by only 20 people in Latvia. The system is based on a rule-based approach and includes a morphological analyzer, a morphological generator, a transfer module, and a target language generator. The system was evaluated using a set of test sentences and achieved an accuracy of 70%. The system has the potential to aid in the preservation and revitalization of Livonian language and culture. \n\n要旨：\n本論文では、ラトビアでわずか20人しか話さない危機に瀕したフィン・ウゴル語族のリヴォニア語の機械翻訳システムの開発を紹介する。このシステムはルールベースのアプローチに基づいており、形態素解析器、形態素生成器、転送モジュール、およびターゲット言語生成器を含んでいる。システムは一連のテスト文を使用して評価され、70％の精度を達成した。このシステムは、リヴォニア語と文化の保存と復興に役立つ可能性がある。", "jabstract": "リヴォニア語は、話者がほとんどおらず、公に利用可能なコーパスもほとんどない、ヨーロッパで最も危機に瀕した言語の1つです。本論文では、リヴォニア語と英語の間のニューラル機械翻訳（NMT）の開発に取り組み、2つの目的を持ちます。一方で、言語を保存し、もう一方で、リヴォニアの民話、人生物語、その他のテキスト上の無形遺産にアクセスしやすくし、さらに並列コーパスを作成しやすくすることです。私たちは、リヴォニア語がエストニア語とラトビア語と言語的に類似していることに依存し、翻訳実験のために4つの言語の並列および単一言語データを収集します。私たちは、ゼロショット翻訳、クロスリンガル転送、合成データ作成などの異なる低リソースNMT技術を組み合わせて、最高の翻訳品質を達成し、どのベース言語がリヴォニア語への転送に実証的により役立つかを見つけます。結果として得られたNMTシステムと収集された単一言語および並列データ、手動で翻訳され検証された翻訳ベンチマークを含むものは、OPUSおよびHuggingfaceリポジトリを通じて公開されます。"}
{"title": "Fire Burns, Sword Cuts: Commonsense Inductive Bias for Exploration in Text-based Games", "url": "https://aclanthology.org/2022.acl-short.56/", "abstract": "Text-based games (TGs) are exciting testbeds for developing deep reinforcement learning techniques due to their partially observed environments and large action spaces. In these games, the agent learns to explore the environment via natural language interactions with the game simulator. A fundamental challenge in TGs is the efficient exploration of the large action space when the agent has not yet acquired enough knowledge about the environment. We propose CommExpl, an exploration technique that injects external commonsense knowledge, via a pretrained language model (LM), into the agent during training when the agent is the most uncertain about its next action. Our method exhibits improvement on the collected game scores during the training in four out of nine games from Jericho. Additionally, the produced trajectory of actions exhibit lower perplexity, when tested with a pretrained LM, indicating better closeness to human language.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "「火は燃える、剣は切る」：テキストベースのゲームにおける探索のための常識的帰納バイアス", "jabstract": "テキストベースのゲーム（TG）は、部分的に観測された環境と大きなアクションスペースのため、深層強化学習技術を開発するための興味深いテストベッドです。これらのゲームでは、エージェントはゲームシミュレータとの自然言語インタラクションを通じて環境を探索することを学習します。TGにおける基本的な課題は、エージェントが環境について十分な知識をまだ獲得していない場合に、大きなアクションスペースを効率的に探索することです。本研究では、エージェントが次のアクションについて最も不確実な場合に、事前学習された言語モデル（LM）を介して外部の常識的な知識を注入する探索技術CommExplを提案します。本手法は、Jerichoの9つのゲームのうち4つのゲームでトレーニング中に収集されたゲームスコアの改善を示しました。さらに、事前学習されたLMでテストした場合、生成されたアクションの軌跡はパープレキシティが低く、人間の言語により近いことを示しています。"}
{"title": "A Simple but Effective Pluggable Entity Lookup Table for Pre-trained Language Models", "url": "https://aclanthology.org/2022.acl-short.57/", "abstract": "Pre-trained language models (PLMs) cannot well recall rich factual knowledge of entities exhibited in large-scale corpora, especially those rare entities. In this paper, we propose to build a simple but effective Pluggable Entity Lookup Table (PELT) on demand by aggregating the entity’s output representations of multiple occurrences in the corpora. PELT can be compatibly plugged as inputs to infuse supplemental entity knowledge into PLMs. Compared to previous knowledge-enhanced PLMs, PELT only requires 0.2%-5% pre-computation with capability of acquiring knowledge from out-of-domain corpora for domain adaptation scenario. The experiments on knowledge-related tasks demonstrate that our method, PELT, can flexibly and effectively transfer entity knowledge from related corpora into PLMs with different architectures. Our code and models are publicly available at https://github.com/thunlp/PELT", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "事前学習済み言語モデルのためのシンプルで効果的なプラグイン可能なエンティティルックアップテーブル", "jabstract": "事前学習言語モデル（PLMs）は、特に稀なエンティティに展示される大規模なコーパスの豊富な事実知識をよく思い出せません。本論文では、複数の出現のエンティティの出力表現を集約して、要求に応じてシンプルで効果的なプラグ可能なエンティティルックアップテーブル（PELT）を構築することを提案します。PELTは、PLMsに補足的なエンティティ知識を注入するための入力として互換性があります。以前の知識強化PLMsと比較して、PELTは、ドメイン適応シナリオのためにドメイン外のコーパスから知識を取得する能力を持ち、0.2％〜5％の事前計算のみが必要です。知識関連のタスクの実験は、PELTが、異なるアーキテクチャを持つPLMsに関連するコーパスからエンティティ知識を柔軟かつ効果的に転送できることを示しています。私たちのコードとモデルは、https://github.com/thunlp/PELTで公開されています。"}
{"title": "S4-Tuning: A Simple Cross-lingual Sub-network Tuning Method", "url": "https://aclanthology.org/2022.acl-short.58/", "abstract": "The emergence of multilingual pre-trained language models makes it possible to adapt to target languages with only few labeled examples.However, vanilla fine-tuning tends to achieve degenerated and unstable results, owing to the Language Interference among different languages, and Parameter Overload under the few-sample transfer learning scenarios.To address two problems elegantly, we propose S4-Tuning, a Simple Cross-lingual Sub-network Tuning method. S4-Tuning first detects the most essential sub-network for each target language, and only updates it during fine-tuning.In this way, the language sub-networks lower the scale of trainable parameters, and hence better suit the low-resource scenarios.Meanwhile, the commonality and characteristics across languages are modeled by the overlapping and non-overlapping parts to ease the interference among languages.Simple but effective, S4-Tuning gains consistent improvements over vanilla fine-tuning on three multi-lingual tasks involving 37 different languages in total (XNLI, PAWS-X, and Tatoeba).", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "S4-Tuning：単純なクロスリンガルサブネットワーク調整法", "jabstract": "多言語事前学習言語モデルの出現により、わずかなラベル付き例でターゲット言語に適応することが可能になりました。しかし、バニラファインチューニングは、異なる言語間の言語干渉と、少数のサンプル転移学習シナリオ下のパラメータオーバーロードにより、退化した不安定な結果を達成する傾向があります。2つの問題に対処するために、我々はS4-Tuningというシンプルなクロスリンガルサブネットチューニング方法を提案します。S4-Tuningは、まず各ターゲット言語に最も重要なサブネットワークを検出し、ファインチューニング中にそれだけを更新します。このように、言語サブネットワークは、トレーニング可能なパラメータのスケールを下げ、低リソースシナリオに適しています。一方、言語間の干渉を緩和するために、重複部分と非重複部分によって言語間の共通性と特徴がモデル化されます。シンプルで効果的なS4-Tuningは、37の異なる言語を含む3つの多言語タスク（XNLI、PAWS-X、Tatoeba）でバニラファインチューニングに比べて一貫した改善を得ています。"}
{"title": "Region-dependent temperature scaling for certainty calibration and application to class-imbalanced token classification", "url": "https://aclanthology.org/2022.acl-short.59/", "abstract": "Certainty calibration is an important goal on the path to interpretability and trustworthy AI. Particularly in the context of human-in-the-loop systems, high-quality low to mid-range certainty estimates are essential. In the presence of a dominant high-certainty class, for instance the non-entity class in NER problems, existing calibration error measures are completely insensitive to potentially large errors in this certainty region of interest. We introduce a region-balanced calibration error metric that weights all certainty regions equally. When low and mid certainty estimates are taken into account, calibration error is typically larger than previously reported. We introduce a simple extension of temperature scaling, requiring no additional computation, that can reduce both traditional and region-balanced notions of calibration error over existing baselines.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "地域に依存した温度スケーリングによる確信度のキャリブレーションと、クラス不均衡トークン分類への適用。", "jabstract": "確信度のキャリブレーションは、解釈可能性と信頼性の高いAIに向けた重要な目標です。特に、人間との連携システムの文脈では、高品質で低〜中程度の確信度の推定が必要不可欠です。例えば、NER問題における非エンティティクラスのように、支配的な高確信度クラスが存在する場合、既存のキャリブレーションエラー測定は、興味のあるこの確信度領域での大きなエラーに完全に無感覚です。私たちは、すべての確信度領域を等しく重み付けする領域バランスの取れたキャリブレーションエラーメトリックを導入します。低〜中程度の確信度推定を考慮すると、キャリブレーションエラーは従来報告されていたよりも大きくなる傾向があります。追加の計算を必要としない温度スケーリングの単純な拡張を導入し、既存のベースラインに対して従来のキャリブレーションエラーと領域バランスの取れたキャリブレーションエラーの両方を削減できます。"}
{"title": "Developmental Negation Processing in Transformer Language Models", "url": "https://aclanthology.org/2022.acl-short.60/", "abstract": "Reasoning using negation is known to be difficult for transformer-based language models. While previous studies have used the tools of psycholinguistics to probe a transformer’s ability to reason over negation, none have focused on the types of negation studied in developmental psychology. We explore how well transformers can process such categories of negation, by framing the problem as a natural language inference (NLI) task. We curate a set of diagnostic questions for our target categories from popular NLI datasets and evaluate how well a suite of models reason over them. We find that models perform consistently better only on certain categories, suggesting clear distinctions in how they are processed.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "トランスフォーマー言語モデルにおける発達的否定処理", "jabstract": "否定を用いた推論は、トランスフォーマーベースの言語モデルにとって困難であることが知られています。以前の研究では、心理言語学のツールを使用してトランスフォーマーが否定に関する推論能力を調べたものの、発達心理学で研究された否定の種類に焦点を当てたものはありませんでした。我々は、自然言語推論（NLI）タスクとして問題をフレーム化し、トランスフォーマーがこのような否定のカテゴリをどの程度処理できるかを探求します。人気のあるNLIデータセットから、対象のカテゴリの診断的な質問を収集し、一連のモデルがそれらを推論する能力を評価します。モデルは、特定のカテゴリに対してのみ一貫して優れたパフォーマンスを発揮し、それらがどのように処理されるかに明確な違いがあることを示唆しています。"}
{"title": "Canary Extraction in Natural Language Understanding Models", "url": "https://aclanthology.org/2022.acl-short.61/", "abstract": "Natural Language Understanding (NLU) models can be trained on sensitive information such as phone numbers, zip-codes etc. Recent literature has focused on Model Inversion Attacks (ModIvA) that can extract training data from model parameters. In this work, we present a version of such an attack by extracting canaries inserted in NLU training data. In the attack, an adversary with open-box access to the model reconstructs the canaries contained in the model’s training set. We evaluate our approach by performing text completion on canaries and demonstrate that by using the prefix (non-sensitive) tokens of the canary, we can generate the full canary. As an example, our attack is able to reconstruct a four digit code in the training dataset of the NLU model with a probability of 0.5 in its best configuration. As countermeasures, we identify several defense mechanisms that, when combined, effectively eliminate the risk of ModIvA in our experiments.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "自然言語処理モデルにおけるカナリア抽出", "jabstract": "自然言語理解（NLU）モデルは、電話番号、郵便番号などの機密情報でトレーニングできます。最近の文献では、モデルパラメータからトレーニングデータを抽出できるモデルインバージョン攻撃（ModIvA）に焦点を当てています。本研究では、NLUトレーニングデータに挿入されたカナリアを抽出することによるそのような攻撃のバージョンを提供します。攻撃では、モデルにオープンボックスアクセスを持つ敵対者が、モデルのトレーニングセットに含まれるカナリアを再構築します。私たちは、カナリアのテキスト補完を実行することによってアプローチを評価し、カナリアのプレフィックス（非機密）トークンを使用することで、完全なカナリアを生成できることを示します。例えば、私たちの攻撃は、最良の構成でNLUモデルのトレーニングデータセット内の4桁のコードを0.5の確率で再構築できます。対策として、私たちはいくつかの防御メカニズムを特定し、実験でModIvAのリスクを効果的に排除することができます。"}
{"title": "On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representations", "url": "https://aclanthology.org/2022.acl-short.62/", "abstract": "Multiple metrics have been introduced to measure fairness in various natural language processing tasks. These metrics can be roughly categorized into two categories: 1) extrinsic metrics for evaluating fairness in downstream applications and 2) intrinsic metrics for estimating fairness in upstream contextualized language representation models. In this paper, we conduct an extensive correlation study between intrinsic and extrinsic metrics across bias notions using 19 contextualized language models. We find that intrinsic and extrinsic metrics do not necessarily correlate in their original setting, even when correcting for metric misalignments, noise in evaluation datasets, and confounding factors such as experiment configuration for extrinsic metrics.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "文脈化言語表現に対する本質的および外在的公平性評価メトリックについて", "jabstract": "自然言語処理の様々なタスクにおける公平性を測定するために、複数のメトリックが導入されています。これらのメトリックは、大まかに2つのカテゴリに分類されます。1つは、下流のアプリケーションにおける公平性を評価するための外的メトリックであり、もう1つは上流の文脈化言語表現モデルにおける公平性を推定するための内的メトリックです。本論文では、19の文脈化言語モデルを用いて、バイアスの概念を横断して内的メトリックと外的メトリックの相関研究を行います。我々は、内的メトリックと外的メトリックが、元の設定において必ずしも相関しないことを発見しました。さらに、メトリックの不一致、評価データセットのノイズ、外的メトリックの実験設定などの混乱要因を修正しても、相関しないことがわかりました。"}
{"title": "Sequence-to-sequence AMR Parsing with Ancestor Information", "url": "https://aclanthology.org/2022.acl-short.63/", "abstract": "AMR parsing is the task that maps a sentence to an AMR semantic graph automatically. The difficulty comes from generating the complex graph structure. The previous state-of-the-art method translates the AMR graph into a sequence, then directly fine-tunes a pretrained sequence-to-sequence Transformer model (BART). However, purely treating the graph as a sequence does not take advantage of structural information about the graph. In this paper, we design several strategies to add the important ancestor information into the Transformer Decoder. Our experiments show that we can improve the performance for both AMR 2.0 and AMR 3.0 dataset and achieve new state-of-the-art results.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "祖先情報を用いたシーケンス・トゥ・シーケンスAMR解析", "jabstract": "AMRパーシングは、文をAMR意味グラフに自動的にマッピングするタスクです。困難な点は、複雑なグラフ構造を生成することです。従来の最先端の方法は、AMRグラフをシーケンスに変換し、事前学習されたシーケンス-シーケンスTransformerモデル（BART）を直接微調整することです。しかし、グラフを単なるシーケンスとして扱うだけでは、グラフの構造情報を活用することができません。本論文では、重要な祖先情報をTransformer Decoderに追加するためのいくつかの戦略を設計しました。実験の結果、AMR 2.0およびAMR 3.0データセットのパフォーマンスを改善し、新しい最先端の結果を達成できることが示されました。"}
{"title": "Zero-Shot Dependency Parsing with Worst-Case Aware Automated Curriculum Learning", "url": "https://aclanthology.org/2022.acl-short.64/", "abstract": "Large multilingual pretrained language models such as mBERT and XLM-RoBERTa have been found to be surprisingly effective for cross-lingual transfer of syntactic parsing models Wu and Dredze (2019), but only between related languages. However, source and training languages are rarely related, when parsing truly low-resource languages. To close this gap, we adopt a method from multi-task learning, which relies on automated curriculum learning, to dynamically optimize for parsing performance on outlier languages. We show that this approach is significantly better than uniform and size-proportional sampling in the zero-shot setting.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "最悪ケースを考慮した自動カリキュラム学習によるゼロショット依存構文解析", "jabstract": "大規模な多言語事前学習言語モデルであるmBERTやXLM-RoBERTaは、Wu and Dredze (2019)によって、関連する言語間での構文解析モデルのクロスリンガル転移に驚くほど効果的であることがわかっています。しかし、解析が本当に低リソースの言語に対して行われる場合、ソースとトレーニング言語はほとんど関連していません。このギャップを埋めるために、自動カリキュラム学習に依存するマルチタスク学習からの手法を採用し、外れ値言語の解析性能を動的に最適化することにしました。私たちは、このアプローチがゼロショット設定で均一およびサイズ比例サンプリングよりも有意に優れていることを示しています。"}
{"title": "PriMock57: A Dataset Of Primary Care Mock Consultations", "url": "https://aclanthology.org/2022.acl-short.65/", "abstract": "Recent advances in Automatic Speech Recognition (ASR) have made it possible to reliably produce automatic transcripts of clinician-patient conversations. However, access to clinical datasets is heavily restricted due to patient privacy, thus slowing down normal research practices. We detail the development of a public access, high quality dataset comprising of 57 mocked primary care consultations, including audio recordings, their manual utterance-level transcriptions, and the associated consultation notes. Our work illustrates how the dataset can be used as a benchmark for conversational medical ASR as well as consultation note generation from transcripts.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "PriMock57: 初期診療モック相談のデータセット\n\n1. Natural language processing (NLP) has the potential to improve healthcare delivery by automating tasks such as triage, diagnosis, and treatment recommendation.\n自然言語処理（NLP）は、トリアージ、診断、治療の推奨などのタスクを自動化することにより、医療提供の改善の可能性を持っています。\n\n2. However, the development of NLP models for healthcare requires large amounts of annotated data, which is often difficult to obtain.\nしかし、医療用NLPモデルの開発には、しばしば入手が困難な注釈付きデータの大量が必要です。\n\n3. In this paper, we present PriMock57, a dataset of primary care mock consultations designed to facilitate the development of NLP models for healthcare.\n本論文では、医療用NLPモデルの開発を促進するために設計された初期診療モック相談のデータセットであるPriMock57を紹介します。", "jabstract": "自然言語処理に関する論文の要約文を日本語に翻訳してください。\n\n自動音声認識（ASR）の最近の進歩により、医師と患者の会話の自動転写を信頼性高く行うことが可能になりました。しかし、患者のプライバシーのために臨床データセットへのアクセスは厳しく制限されており、通常の研究活動が遅れています。本研究では、音声録音、手動発話レベルの転写、関連する診療ノートを含む57件の模擬一次診療相談からなる公開アクセス可能な高品質データセットの開発について詳細に説明します。このデータセットは、会話型医療ASRのベンチマークとして、また転写からの診療ノート生成のためのベンチマークとして使用できることを示しています。"}
{"title": "UniGDD: A Unified Generative Framework for Goal-Oriented Document-Grounded Dialogue", "url": "https://aclanthology.org/2022.acl-short.66/", "abstract": "The goal-oriented document-grounded dialogue aims at responding to the user query based on the dialogue context and supporting document. Existing studies tackle this problem by decomposing it into two sub-tasks: knowledge identification and response generation. However, such pipeline methods would unavoidably suffer from the error propagation issue. This paper proposes to unify these two sub-tasks via sequentially generating the grounding knowledge and the response. We further develop a prompt-connected multi-task learning strategy to model the characteristics and connections of different tasks and introduce linear temperature scheduling to reduce the negative effect of irrelevant document information. Experimental results demonstrate the effectiveness of our framework.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "UniGDD：目的指向型文書基盤対話のための統一生成フレームワーク", "jabstract": "目的指向型の文書に基づく対話は、対話コンテキストとサポート文書に基づいてユーザーのクエリに応答することを目的としています。既存の研究は、この問題を2つのサブタスク、知識の特定と応答生成に分解して対処しています。しかし、このようなパイプライン方式は、エラー伝播の問題に必然的に直面することになります。本論文では、接地知識と応答を順次生成することによって、これら2つのサブタスクを統合することを提案しています。さらに、異なるタスクの特性と接続をモデル化するためのプロンプト接続マルチタスク学習戦略を開発し、線形温度スケジューリングを導入して、関連のない文書情報の負の影響を軽減します。実験結果は、我々のフレームワークの有効性を示しています。"}
{"title": "DMix: Adaptive Distance-aware Interpolative Mixup", "url": "https://aclanthology.org/2022.acl-short.67/", "abstract": "Interpolation-based regularisation methods such as Mixup, which generate virtual training samples, have proven to be effective for various tasks and modalities.We extend Mixup and propose DMix, an adaptive distance-aware interpolative Mixup that selects samples based on their diversity in the embedding space. DMix leverages the hyperbolic space as a similarity measure among input samples for a richer encoded representation.DMix achieves state-of-the-art results on sentence classification over existing data augmentation methods on 8 benchmark datasets across English, Arabic, Turkish, and Hindi languages while achieving benchmark F1 scores in 3 times less number of iterations.We probe the effectiveness of DMix in conjunction with various similarity measures and qualitatively analyze the different components.DMix being generalizable, can be applied to various tasks, models and modalities.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "DMix：適応的な距離感知補間Mixup", "jabstract": "Mixupのような補間ベースの正則化手法は、仮想トレーニングサンプルを生成することで、さまざまなタスクやモダリティに効果的であることが証明されています。私たちはMixupを拡張し、埋め込み空間における多様性に基づいてサンプルを選択する適応的な距離感知補間MixupであるDMixを提案します。DMixは、より豊富な符号化表現のために、入力サンプル間の類似性尺度として双曲空間を利用します。DMixは、英語、アラビア語、トルコ語、ヒンディー語の8つのベンチマークデータセットにおいて、既存のデータ拡張手法に比べて文分類において最先端の結果を達成し、3倍少ない反復回数でベンチマークF1スコアを達成します。私たちは、さまざまな類似性尺度との組み合わせでDMixの効果を調べ、異なるコンポーネントを定性的に分析します。DMixは汎用性があり、さまざまなタスク、モデル、モダリティに適用することができます。"}
{"title": "Sub-Word Alignment is Still Useful: A Vest-Pocket Method for Enhancing Low-Resource Machine Translation", "url": "https://aclanthology.org/2022.acl-short.68/", "abstract": "We leverage embedding duplication between aligned sub-words to extend the Parent-Child transfer learning method, so as to improve low-resource machine translation. We conduct experiments on benchmark datasets of My-En, Id-En and Tr-En translation scenarios. The test results show that our method produces substantial improvements, achieving the BLEU scores of 22.5, 28.0 and 18.1 respectively. In addition, the method is computationally efficient which reduces the consumption of training time by 63.8%, reaching the duration of 1.6 hours when training on a Tesla 16GB P100 GPU. All the models and source codes in the experiments will be made publicly available to support reproducible research.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "サブワードアラインメントはまだ有用である：低リソース機械翻訳を強化するためのポケットサイズの方法", "jabstract": "私たちは、自然言語処理に関する論文の要約を以下に示します。\n\n私たちは、アラインされたサブワード間の埋め込みの重複を活用して、親子転移学習法を拡張し、低リソースの機械翻訳を改善することを目的としています。私たちは、My-En、Id-En、Tr-Enのベンチマークデータセットで実験を行いました。テスト結果は、私たちの方法が大幅な改善をもたらし、BLEUスコアがそれぞれ22.5、28.0、18.1に達したことを示しています。さらに、この方法は計算効率が高く、トレーニング時間の消費を63.8%削減し、Tesla 16GB P100 GPUでトレーニングする場合、1.6時間の期間に達します。実験で使用されたすべてのモデルとソースコードは、再現可能な研究をサポートするために公開されます。"}
{"title": "HYPHEN: Hyperbolic Hawkes Attention For Text Streams", "url": "https://aclanthology.org/2022.acl-short.69/", "abstract": "Analyzing the temporal sequence of texts from sources such as social media, news, and parliamentary debates is a challenging problem as it exhibits time-varying scale-free properties and fine-grained timing irregularities. We propose a Hyperbolic Hawkes Attention Network (HYPHEN), which learns a data-driven hyperbolic space and models irregular powerlaw excitations using a hyperbolic Hawkes process. Through quantitative and exploratory experiments over financial NLP, suicide ideation detection, and political debate analysis we demonstrate HYPHEN’s practical applicability for modeling online text sequences in a geometry agnostic manner.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "HYPHEN：テキストストリームのための双曲線ホークスアテンション", "jabstract": "ソーシャルメディア、ニュース、議会の議論などのテキストの時間的なシーケンスを分析することは、時間変化するスケールフリーな特性や細かいタイミングの不規則性を示すため、課題があります。本論文では、データ駆動型の双曲線空間を学習し、双曲線ホークス過程を用いて不規則なパワーロー励起をモデル化するHyperbolic Hawkes Attention Network（HYPHEN）を提案します。金融NLP、自殺思考検出、政治的議論分析における定量的および探索的実験を通じて、HYPHENがジオメトリに無関係な方法でオンラインテキストシーケンスをモデル化するための実用的な適用性を示します。"}
{"title": "A Risk-Averse Mechanism for Suicidality Assessment on Social Media", "url": "https://aclanthology.org/2022.acl-short.70/", "abstract": "Recent studies have shown that social media has increasingly become a platform for users to express suicidal thoughts outside traditional clinical settings. With advances in Natural Language Processing strategies, it is now possible to design automated systems to assess suicide risk. However, such systems may generate uncertain predictions, leading to severe consequences. We hence reformulate suicide risk assessment as a selective prioritized prediction problem over the Columbia Suicide Severity Risk Scale (C-SSRS). We propose SASI, a risk-averse and self-aware transformer-based hierarchical attention classifier, augmented to refrain from making uncertain predictions. We show that SASI is able to refrain from 83% of incorrect predictions on real-world Reddit data. Furthermore, we discuss the qualitative, practical, and ethical aspects of SASI for suicide risk assessment as a human-in-the-loop framework.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "自然言語処理に関する論文の要約文を日本語に翻訳してください。\n\nソーシャルメディア上での自殺リスク評価のためのリスク回避メカニズム", "jabstract": "最近の研究により、ソーシャルメディアが従来の臨床設定外で自殺的な思考を表現するためのプラットフォームとしてますます重要になっていることが示されています。自然言語処理戦略の進歩により、自殺リスクを評価する自動システムを設計することが可能になりました。しかし、そのようなシステムは不確実な予測を生成する可能性があり、深刻な結果をもたらす可能性があります。したがって、私たちは自殺リスク評価をコロンビア自殺重症度リスクスケール（C-SSRS）の選択的優先予測問題として再定式化します。私たちは、不確実な予測を避けるために拡張されたリスク回避型自己認識トランスフォーマーベースの階層的注意分類器であるSASIを提案します。私たちは、SASIが実世界のRedditデータで83％の不正確な予測を避けることができることを示します。さらに、SASIを人間イン・ザ・ループフレームワークとして自殺リスク評価に使用する際の質的、実用的、倫理的側面について議論します。"}
{"title": "When classifying grammatical role, BERT doesn’t care about word order... except when it matters", "url": "https://aclanthology.org/2022.acl-short.71/", "abstract": "Because meaning can often be inferred from lexical semantics alone, word order is often a redundant cue in natural language. For example, the words chopped, chef, and onion are more likely used to convey “The chef chopped the onion,” not “The onion chopped the chef.” Recent work has shown large language models to be surprisingly word order invariant, but crucially has largely considered natural prototypical inputs, where compositional meaning mostly matches lexical expectations. To overcome this confound, we probe grammatical role representation in English BERT and GPT-2, on instances where lexical expectations are not sufficient, and word order knowledge is necessary for correct classification. Such non-prototypical instances are naturally occurring English sentences with inanimate subjects or animate objects, or sentences where we systematically swap the arguments to make sentences like “The onion chopped the chef”. We find that, while early layer embeddings are largely lexical, word order is in fact crucial in defining the later-layer representations of words in semantically non-prototypical positions. Our experiments isolate the effect of word order on the contextualization process, and highlight how models use context in the uncommon, but critical, instances where it matters.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "文法的な役割を分類する際、BERTは単語の順序には関心を持ちません... ただし、重要な場合は例外です。", "jabstract": "自然言語において、意味はしばしば語彙意味だけから推測できるため、単語の順序はしばしば冗長な手がかりとなる。例えば、「chopped」、「chef」、「onion」という単語は、「The chef chopped the onion」を伝えるためにより頻繁に使用されるが、「The onion chopped the chef」という意味を伝えるためには使用されない。最近の研究では、大規模な言語モデルが驚くほど単語の順序に依存しないことが示されているが、重要なのは、主に構成的な意味が語彙の期待にほぼ一致する自然な典型的な入力について考慮されていることである。この混乱を克服するために、英語BERTとGPT-2の文法的役割表現を調べ、語彙の期待だけでは不十分で、単語の順序の知識が正しい分類に必要な場合について調べる。このような非典型的なインスタンスは、無生物主語または生物目的語を持つ自然発生的な英語の文、または「The onion chopped the chef」という文を作るために引数を系統的に交換する文である。私たちは、初期のレイヤーの埋め込みが主に語彙的である一方、単語の順序が意味的に非典型的な位置にある単語の後のレイヤー表現を定義する上で実際に重要であることを発見した。私たちの実験は、文脈化プロセスにおける単語の順序の影響を分離し、モデルが重要な場合にのみ文脈を使用する方法を強調している。"}
{"title": "Triangular Transfer: Freezing the Pivot for Triangular Machine Translation", "url": "https://aclanthology.org/2022.acl-short.72/", "abstract": "Triangular machine translation is a special case of low-resource machine translation where the language pair of interest has limited parallel data, but both languages have abundant parallel data with a pivot language. Naturally, the key to triangular machine translation is the successful exploitation of such auxiliary data. In this work, we propose a transfer-learning-based approach that utilizes all types of auxiliary data. As we train auxiliary source-pivot and pivot-target translation models, we initialize some parameters of the pivot side with a pre-trained language model and freeze them to encourage both translation models to work in the same pivot language space, so that they can be smoothly transferred to the source-target translation model. Experiments show that our approach can outperform previous ones.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "自然言語処理に関する論文の要約文を日本語に翻訳します。\n\n三角形転送：三角形機械翻訳のためのピボットの凍結", "jabstract": "三角形機械翻訳は、関心のある言語ペアに限られた並列データがあるが、両言語にはピボット言語との豊富な並列データがある低リソース機械翻訳の特別な場合です。自然に、三角形機械翻訳の鍵は、そのような補助データの成功した利用です。本研究では、すべての種類の補助データを利用する転移学習ベースのアプローチを提案します。補助ソース-ピボットおよびピボット-ターゲット翻訳モデルをトレーニングする際に、ピボット側の一部のパラメータを事前にトレーニングされた言語モデルで初期化し、両方の翻訳モデルが同じピボット言語空間で動作するように凍結して、ソース-ターゲット翻訳モデルにスムーズに転送できるようにします。実験の結果、当社のアプローチは以前のアプローチを上回ることが示されました。"}
{"title": "Can Visual Dialogue Models Do Scorekeeping? Exploring How Dialogue Representations Incrementally Encode Shared Knowledge", "url": "https://aclanthology.org/2022.acl-short.73/", "abstract": "Cognitively plausible visual dialogue models should keep a mental scoreboard of shared established facts in the dialogue context. We propose a theory-based evaluation method for investigating to what degree models pretrained on the VisDial dataset incrementally build representations that appropriately do scorekeeping. Our conclusion is that the ability to make the distinction between shared and privately known statements along the dialogue is moderately present in the analysed models, but not always incrementally consistent, which may partially be due to the limited need for grounding interactions in the original task.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "視覚対話モデルはスコアキーピングを行えるか？ 対話表現が共有知識を段階的にエンコードする方法を探る。", "jabstract": "認知的に妥当な視覚的対話モデルは、対話コンテキストで共有された確立された事実のメンタルスコアボードを保持する必要があります。私たちは、VisDialデータセットで事前にトレーニングされたモデルが適切にスコアキーピングを行う表現を増分的に構築する度合いを調査するための理論に基づく評価方法を提案します。私たちの結論は、対話の共有されたと個人的に知られている声明の区別を行う能力が分析されたモデルには適度に存在するが、常に増分的に一貫していないことであり、これは元のタスクでの接地相互作用の限定された必要性に部分的に起因する可能性がある。"}
{"title": "Focus on the Target’s Vocabulary: Masked Label Smoothing for Machine Translation", "url": "https://aclanthology.org/2022.acl-short.74/", "abstract": "Label smoothing and vocabulary sharing are two widely used techniques in neural machine translation models. However, we argue that simply applying both techniques can be conflicting and even leads to sub-optimal performance. When allocating smoothed probability, original label smoothing treats the source-side words that would never appear in the target language equally to the real target-side words, which could bias the translation model. To address this issue, we propose Masked Label Smoothing (MLS), a new mechanism that masks the soft label probability of source-side words to zero. Simple yet effective, MLS manages to better integrate label smoothing with vocabulary sharing. Our extensive experiments show that MLS consistently yields improvement over original label smoothing on different datasets, including bilingual and multilingual translation from both translation quality and model’s calibration. Our code is released at https://github.com/PKUnlp-icler/MLS", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "ターゲットの語彙に注目：機械翻訳のためのマスクされたラベルスムージング", "jabstract": "ラベルスムージングと語彙共有は、ニューラル機械翻訳モデルで広く使用されている2つの技術です。しかし、両方の技術を単純に適用することは、矛盾して最適なパフォーマンスになる可能性があります。スムージングされた確率を割り当てる際、オリジナルのラベルスムージングは、ターゲット言語には決して現れないソース側の単語を実際のターゲット側の単語と同等に扱います。これは、翻訳モデルにバイアスを与える可能性があります。この問題に対処するために、私たちはマスクされたラベルスムージング（MLS）という新しいメカニズムを提案しています。MLSは、ソース側の単語のソフトラベル確率をゼロにマスクすることで、ラベルスムージングと語彙共有をより良く統合することができます。私たちの広範な実験は、MLSが、バイリンガルおよびマルチリンガル翻訳を含むさまざまなデータセットで、翻訳品質とモデルのキャリブレーションの両方で、オリジナルのラベルスムージングよりも一貫して改善をもたらすことを示しています。私たちのコードは、https://github.com/PKUnlp-icler/MLSで公開されています。"}
{"title": "Contrastive Learning-Enhanced Nearest Neighbor Mechanism for Multi-Label Text Classification", "url": "https://aclanthology.org/2022.acl-short.75/", "abstract": "Multi-Label Text Classification (MLTC) is a fundamental and challenging task in natural language processing. Previous studies mainly focus on learning text representation and modeling label correlation but neglect the rich knowledge from the existing similar instances when predicting labels of a specific text. To make up for this oversight, we propose a k nearest neighbor (kNN) mechanism which retrieves several neighbor instances and interpolates the model output with their labels. Moreover, we design a multi-label contrastive learning objective that makes the model aware of the kNN classification process and improves the quality of the retrieved neighbors while inference. Extensive experiments show that our method can bring consistent and significant performance improvement to multiple MLTC models including the state-of-the-art pretrained and non-pretrained ones.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "多ラベルテキスト分類のための対比学習強化最近傍メカニズム", "jabstract": "マルチラベルテキスト分類（MLTC）は、自然言語処理における基本的かつ難解なタスクです。従来の研究は、主にテキスト表現の学習とラベル相関のモデリングに焦点を当てていますが、特定のテキストのラベルを予測する際に既存の類似インスタンスから得られる豊富な知識を無視しています。この見落としを補うために、私たちはk最近傍（kNN）メカニズムを提案し、いくつかの近隣インスタンスを取得し、そのラベルでモデル出力を補間します。さらに、kNN分類プロセスに気付かせ、推論中に取得される近隣の品質を向上させるマルチラベル対照学習目的を設計しました。広範な実験により、私たちの方法は、事前学習済みおよび非事前学習済みの最新のMLTCモデルを含む複数のモデルに一貫してかつ有意な性能向上をもたらすことが示されました。"}
{"title": "NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better", "url": "https://aclanthology.org/2022.acl-short.76/", "abstract": "Effectively finetuning pretrained language models (PLMs) is critical for their success in downstream tasks. However, PLMs may have risks in overfitting the pretraining tasks and data, which usually have gap with the target downstream tasks. Such gap may be difficult for existing PLM finetuning methods to overcome and lead to suboptimal performance. In this paper, we propose a very simple yet effective method named NoisyTune to help better finetune PLMs on downstream tasks by adding some noise to the parameters of PLMs before fine-tuning. More specifically, we propose a matrix-wise perturbing method which adds different uniform noises to different parameter matrices based on their standard deviations. In this way, the varied characteristics of different types of parameters in PLMs can be considered. Extensive experiments on both GLUE English benchmark and XTREME multilingual benchmark show NoisyTune can consistently empower the finetuning of different PLMs on different downstream tasks.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "NoisyTune：少しのノイズが事前学習済み言語モデルの微調整をより良くするのに役立つ", "jabstract": "事前学習済み言語モデル（PLMs）を効果的にファインチューニングすることは、下流タスクでの成功にとって重要です。しかし、PLMsは、事前学習タスクとデータとのギャップが下流タスクと異なるため、過学習のリスクがあります。このようなギャップは、既存のPLMファインチューニング手法が克服するのが難しく、最適なパフォーマンスにつながらない可能性があります。本論文では、PLMsのパラメータにノイズを加えることで、下流タスクでのPLMsのファインチューニングをより良くするための非常にシンプルで効果的な方法であるNoisyTuneを提案します。具体的には、標準偏差に基づいて異なる一様ノイズを異なるパラメータ行列に加える行列単位の摂動法を提案します。これにより、PLMsの異なるタイプのパラメータの多様な特性を考慮できます。GLUE英語ベンチマークとXTREME多言語ベンチマークの両方での広範な実験結果は、NoisyTuneが異なるPLMsを異なる下流タスクで一貫して強化できることを示しています。"}
{"title": "Adjusting the Precision-Recall Trade-Off with Align-and-Predict Decoding for Grammatical Error Correction", "url": "https://aclanthology.org/2022.acl-short.77/", "abstract": "Modern writing assistance applications are always equipped with a Grammatical Error Correction (GEC) model to correct errors in user-entered sentences. Different scenarios have varying requirements for correction behavior, e.g., performing more precise corrections (high precision) or providing more candidates for users (high recall). However, previous works adjust such trade-off only for sequence labeling approaches. In this paper, we propose a simple yet effective counterpart – Align-and-Predict Decoding (APD) for the most popular sequence-to-sequence models to offer more flexibility for the precision-recall trade-off. During inference, APD aligns the already generated sequence with input and adjusts scores of the following tokens. Experiments in both English and Chinese GEC benchmarks show that our approach not only adapts a single model to precision-oriented and recall-oriented inference, but also maximizes its potential to achieve state-of-the-art results. Our code is available at https://github.com/AutoTemp/Align-and-Predict.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "「Align-and-Predict Decoding」を用いた「文法エラー訂正」における「適合率-再現率トレードオフ」の調整方法についての論文の要旨です。", "jabstract": "現代の文章補助アプリケーションは、ユーザーが入力した文章の文法エラーを修正するための文法エラー修正（GEC）モデルを常に備えています。異なるシナリオには、より正確な修正（高精度）を行うか、ユーザーにより多くの候補を提供するか（高リコール）など、異なる要件があります。しかし、以前の研究では、このようなトレードオフをシーケンスラベリングアプローチに対してのみ調整していました。本論文では、最も一般的なシーケンス・トゥ・シーケンス・モデルに対して、シンプルで効果的な対応策である「Align-and-Predict Decoding（APD）」を提案し、精度とリコールのトレードオフに対してより柔軟性を提供します。推論中、APDは既に生成されたシーケンスを入力と整列し、後続のトークンのスコアを調整します。英語と中国語のGECベンチマークでの実験結果は、当社のアプローチが単一のモデルを精度指向とリコール指向の推論に適応させるだけでなく、最先端の結果を達成するための潜在能力を最大限に引き出すことができることを示しています。当社のコードはhttps://github.com/AutoTemp/Align-and-Predictで入手できます。"}
{"title": "On the Effect of Isotropy on VAE Representations of Text", "url": "https://aclanthology.org/2022.acl-short.78/", "abstract": "Injecting desired geometric properties into text representations has attracted a lot of attention. A property that has been argued for, due to its better utilisation of representation space, is isotropy. In parallel, VAEs have been successful in areas of NLP, but are known for their sub-optimal utilisation of the representation space. To address an aspect of this, we investigate the impact of injecting isotropy during training of VAEs. We achieve this by using an isotropic Gaussian posterior (IGP) instead of the ellipsoidal Gaussian posterior. We illustrate that IGP effectively encourages isotropy in the representations, inducing a more discriminative latent space. Compared to vanilla VAE, this translates into a much better classification performance, robustness to input perturbation, and generative behavior. Additionally, we offer insights about the representational properties encouraged by IGP.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "自然言語処理に関する論文の要旨を日本語に翻訳します。\n\nテキストのVAE表現における等方性の影響について", "jabstract": "所望の幾何学的特性をテキスト表現に注入することは、多くの注目を集めています。表現空間のより良い利用のために主張されている特性の1つは等方性です。同時に、VAEはNLPの領域で成功していますが、表現空間のサブオプティマルな利用で知られています。この問題の一部に対処するために、VAEのトレーニング中に等方性を注入する影響を調査しました。これは、楕円形のガウス事後分布の代わりに等方性のガウス事後分布（IGP）を使用することによって達成されます。IGPは、表現に等方性を効果的に促進し、より識別的な潜在空間を誘発します。バニラVAEと比較して、これは分類性能、入力の摂動に対する堅牢性、および生成的な振る舞いにおいてはるかに優れた結果に翻訳されます。さらに、IGPによって促進される表現的特性についての洞察を提供します。"}
{"title": "Efficient Classification of Long Documents Using Transformers", "url": "https://aclanthology.org/2022.acl-short.79/", "abstract": "Several methods have been proposed for classifying long textual documents using Transformers. However, there is a lack of consensus on a benchmark to enable a fair comparison among different approaches. In this paper, we provide a comprehensive evaluation of the relative efficacy measured against various baselines and diverse datasets — both in terms of accuracy as well as time and space overheads. Our datasets cover binary, multi-class, and multi-label classification tasks and represent various ways information is organized in a long text (e.g. information that is critical to making the classification decision is at the beginning or towards the end of the document). Our results show that more complex models often fail to outperform simple baselines and yield inconsistent performance across datasets. These findings emphasize the need for future studies to consider comprehensive baselines and datasets that better represent the task of long document classification to develop robust models.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "トランスフォーマーを使用した長文書の効率的な分類", "jabstract": "トランスフォーマーを使用した長いテキスト文書の分類には、いくつかの方法が提案されています。しかし、異なるアプローチを公平に比較するためのベンチマークの合意が欠けています。本論文では、正確性だけでなく時間とスペースのオーバーヘッドに対する相対的な効果を、さまざまなベースラインとデータセットに対して包括的に評価します。当社のデータセットは、バイナリ、マルチクラス、およびマルチラベル分類タスクをカバーし、長いテキスト内の情報がどのように組織されているかを表します（例えば、分類決定に重要な情報が文書の最初または最後にある場合など）。当社の結果は、より複雑なモデルがしばしば単純なベースラインを上回ることができず、データセット全体で一貫したパフォーマンスを発揮しないことを示しています。これらの結果は、堅牢なモデルを開発するために、包括的なベースラインと長い文書分類のタスクをよりよく表すデータセットを検討するために将来の研究が必要であることを強調しています。"}
{"title": "Rewarding Semantic Similarity under Optimized Alignments for AMR-to-Text Generation", "url": "https://aclanthology.org/2022.acl-short.80/", "abstract": "A common way to combat exposure bias is by applying scores from evaluation metrics as rewards in reinforcement learning (RL). Metrics leveraging contextualized embeddings appear more flexible than their n-gram matching counterparts and thus ideal as training rewards. However, metrics such as BERTScore greedily align candidate and reference tokens, which can allow system outputs to receive excess credit relative to a reference. Furthermore, past approaches featuring semantic similarity rewards suffer from repetitive outputs and overfitting. We address these issues by proposing metrics that replace the greedy alignments in BERTScore with optimized ones. We compute them on a model’s trained token embeddings to prevent domain mismatch. Our model optimizing discrete alignment metrics consistently outperforms cross-entropy and BLEU reward baselines on AMR-to-text generation. In addition, we find that this approach enjoys stable training compared to a non-RL setting.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "AMR-to-Text生成のための最適化されたアラインメント下での意味的類似性の報酬化", "jabstract": "露出バイアスに対処する一般的な方法は、強化学習（RL）において評価メトリックからスコアを適用することです。文脈化埋め込みを活用するメトリックは、n-gramマッチングの対応物よりも柔軟性が高く、トレーニング報酬として理想的です。ただし、BERTScoreなどのメトリックは、候補と参照トークンを貪欲に整列させるため、システム出力が参照に比べて過剰なクレジットを受ける可能性があります。さらに、過去の意味的類似性報酬を特徴とするアプローチは、繰り返しの出力や過学習に苦しんでいます。私たちは、BERTScoreの貪欲な整列を最適化されたものに置き換えるメトリックを提案することで、これらの問題に対処します。ドメインミスマッチを防ぐために、モデルのトレーニングされたトークン埋め込みで計算します。離散整列メトリックを最適化する私たちのモデルは、AMR-to-text生成において、クロスエントロピーとBLEU報酬のベースラインを一貫して上回ります。さらに、このアプローチは、非RL設定に比べて安定したトレーニングを実現することがわかりました。"}
{"title": "An Analysis of Negation in Natural Language Understanding Corpora", "url": "https://aclanthology.org/2022.acl-short.81/", "abstract": "This paper analyzes negation in eight popular corpora spanning six natural language understanding tasks. We show that these corpora have few negations compared to general-purpose English, and that the few negations in them are often unimportant. Indeed, one can often ignore negations and still make the right predictions. Additionally, experimental results show that state-of-the-art transformers trained with these corpora obtain substantially worse results with instances that contain negation, especially if the negations are important. We conclude that new corpora accounting for negation are needed to solve natural language understanding tasks when negation is present.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "自然言語処理における否定の分析についての論文の要旨です。\n\nNote: As an AI language model, I cannot provide a guarantee of accuracy for my translations.", "jabstract": "この論文は、6つの自然言語理解タスクをカバーする8つの人気コーパスにおける否定の分析を行っています。これらのコーパスは、一般的な英語に比べて否定が少なく、それらのうちの少数の否定はしばしば重要ではないことが示されています。実際、否定を無視しても正しい予測ができることがあります。さらに、実験結果は、これらのコーパスで訓練された最新のトランスフォーマーが、否定が含まれる場合、特に否定が重要な場合には、大幅に悪い結果を示すことを示しています。否定が存在する場合に自然言語理解タスクを解決するためには、否定を考慮した新しいコーパスが必要であると結論付けています。"}
{"title": "Primum Non Nocere: Before working with Indigenous data, the ACL must confront ongoing colonialism", "url": "https://aclanthology.org/2022.acl-short.82/", "abstract": "In this paper, we challenge the ACL community to reckon with historical and ongoing colonialism by adopting a set of ethical obligations and best practices drawn from the Indigenous studies literature. While the vast majority of NLP research focuses on a very small number of very high resource languages (English, Chinese, etc), some work has begun to engage with Indigenous languages. No research involving Indigenous language data can be considered ethical without first acknowledging that Indigenous languages are not merely very low resource languages. The toxic legacy of colonialism permeates every aspect of interaction between Indigenous communities and outside researchers. To this end, we propose that the ACL draft and adopt an ethical framework for NLP researchers and computational linguists wishing to engage in research involving Indigenous languages.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "プリウム・ノン・ノケレ：先住民のデータを扱う前に、ACLは継続的な植民地主義に直面しなければならない。\n\nNatural language processing has made significant progress in recent years, but there is still much work to be done to ensure that it is inclusive and respectful of all languages and cultures.\n\n自然言語処理は近年、大きな進歩を遂げていますが、すべての言語や文化に包括的かつ尊重されるようにするためには、まだ多くの作業が必要です。", "jabstract": "この論文では、先住民研究文献から引用された一連の倫理的義務とベストプラクティスを採用することにより、ACLコミュニティに歴史的および継続的な植民地主義に対処するよう求めます。NLP研究のほとんどは、非常に高いリソースを持つごく少数の言語（英語、中国語など）に焦点を当てていますが、一部の研究では先住民言語に取り組むことが始まっています。先住民言語データを用いた研究においては、先住民言語が単に非常に低いリソース言語であるだけでなく、植民地主義の有害な遺産が先住民コミュニティと外部研究者の相互作用のあらゆる側面に浸透していることを認めることが必要です。このため、私たちはACLが、先住民言語に関する研究に従事するNLP研究者や計算言語学者が採用する倫理的枠組みを起草し、採用することを提案します。"}
{"title": "Unsupervised multiple-choice question generation for out-of-domain Q&A fine-tuning", "url": "https://aclanthology.org/2022.acl-short.83/", "abstract": "Pre-trained models have shown very good performances on a number of question answering benchmarks especially when fine-tuned on multiple question answering datasets at once. In this work, we propose an approach for generating a fine-tuning dataset thanks to a rule-based algorithm that generates questions and answers from unannotated sentences. We show that the state-of-the-art model UnifiedQA can greatly benefit from such a system on a multiple-choice benchmark about physics, biology and chemistry it has never been trained on. We further show that improved performances may be obtained by selecting the most challenging distractors (wrong answers), with a dedicated ranker based on a pretrained RoBERTa model.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "ドメイン外のQ&Aファインチューニングのための非監視多肢選択問題生成", "jabstract": "事前学習済みモデルは、特に複数の質問応答データセットで微調整された場合、多数の質問応答ベンチマークで非常に良いパフォーマンスを示しています。本研究では、未注釈の文から質問と回答を生成するルールベースのアルゴリズムにより、微調整データセットを生成するアプローチを提案します。我々は、UnifiedQAという最先端のモデルが、物理学、生物学、化学に関する多肢選択問題のベンチマークにおいて、これまでトレーニングされたことがないデータセットから、このようなシステムから大きな恩恵を受けることを示します。さらに、事前学習済みRoBERTaモデルに基づく専用のランカーを使用して、最も難しい誘惑者（誤った回答）を選択することで、改善されたパフォーマンスが得られることを示します。"}
{"title": "Can a Transformer Pass the Wug Test? Tuning Copying Bias in Neural Morphological Inflection Models", "url": "https://aclanthology.org/2022.acl-short.84/", "abstract": "Deep learning sequence models have been successful with morphological inflection generation. The SIGMORPHON shared task results in the past several years indicate that such models can perform well, but only if the training data covers a good amount of different lemmata, or if the lemmata to be inflected at test time have also been seen in training, as has indeed been largely the case in these tasks. Surprisingly, we find that standard models such as the Transformer almost completely fail at generalizing inflection patterns when trained on a limited number of lemmata and asked to inflect previously unseen lemmata—i.e. under “wug test”-like circumstances. This is true even though the actual number of training examples is very large. While established data augmentation techniques can be employed to alleviate this shortcoming by introducing a copying bias through hallucinating synthetic new word forms using the alphabet in the language at hand, our experiment results show that, to be more effective, the hallucination process needs to pay attention to substrings of syllable-like length rather than individual characters.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "トランスフォーマーはWugテストに合格できるか？ニューラル形態変化モデルにおけるコピーのバイアス調整", "jabstract": "形態変化生成において、深層学習シーケンスモデルは成功しています。過去数年間のSIGMORPHON共有タスクの結果は、そのようなモデルがうまく機能することを示していますが、トレーニングデータがさまざまな語彙をカバーしている場合、またはテスト時に屈折する語彙がトレーニングで見られた場合に限ります。驚くべきことに、トランスフォーマーなどの標準的なモデルは、限られた数の語彙でトレーニングされ、以前に見たことのない語彙を屈折するように求められた場合、ほとんど完全に屈折パターンを一般化することができません。これは、実際のトレーニング例の数が非常に多いにもかかわらず、真実です。確立されたデータ拡張技術を使用して、言語のアルファベットを使用して合成的な新しい単語形を幻想することで、この欠点を緩和することができますが、私たちの実験結果は、より効果的にするために、幻想プロセスが個々の文字ではなく、音節のような長さのサブストリングに注意を払う必要があることを示しています。"}
{"title": "Probing the Robustness of Trained Metrics for Conversational Dialogue Systems", "url": "https://aclanthology.org/2022.acl-short.85/", "abstract": "This paper introduces an adversarial method to stress-test trained metrics for the evaluation of conversational dialogue systems. The method leverages Reinforcement Learning to find response strategies that elicit optimal scores from the trained metrics. We apply our method to test recently proposed trained metrics. We find that they all are susceptible to giving high scores to responses generated by rather simple and obviously flawed strategies that our method converges on. For instance, simply copying parts of the conversation context to form a response yields competitive scores or even outperforms responses written by humans.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "会話型対話システムのトレーニング済みメトリックの堅牢性を調査する。", "jabstract": "この論文では、会話型対話システムの評価のためのトレーニング済みメトリックをストレステストするための敵対的な方法を紹介しています。この方法は、強化学習を活用して、トレーニング済みメトリックから最適なスコアを引き出す応答戦略を見つけます。我々は、最近提案されたトレーニング済みメトリックをテストするためにこの方法を適用しました。我々は、我々の方法が収束する比較的単純で明らかに欠陥のある戦略によって生成された応答に高いスコアを与える傾向があることを発見しました。例えば、単に会話の文脈の一部をコピーして応答を形成することで、競争力のあるスコアを得ることができ、人間が書いた応答を上回ることさえあります。"}
{"title": "Rethinking and Refining the Distinct Metric", "url": "https://aclanthology.org/2022.acl-short.86/", "abstract": "Distinct is a widely used automatic metric for evaluating diversity in language generation tasks.However, we observed that the original approach to calculating distinct scores has evident biases that tend to assign higher penalties to longer sequences. We refine the calculation of distinct scores by scaling the number of distinct tokens based on their expectations. We provide both empirical and theoretical evidence to show that our method effectively removes the biases existing in the original distinct score. Our experiments show that our proposed metric, Expectation-Adjusted Distinct (EAD), correlates better with human judgment in evaluating response diversity.To assist future research, we provide an example implementation at https://github.com/lsy641/Expectation-Adjusted-Distinct.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "for Natural Language Processing\n\n自然言語処理における独自のメトリックを再考し、洗練すること", "jabstract": "「Distinct」は、言語生成タスクにおける多様性を評価するために広く使用されている自動評価指標である。しかし、我々は、distinctスコアを計算するための元のアプローチには、より長いシーケンスに対してより高いペナルティを割り当てる傾向がある明白なバイアスが存在することを観察した。我々は、期待値に基づいて異なるトークンの数をスケーリングすることによって、distinctスコアの計算を改良した。我々は、我々の方法が元のdistinctスコアに存在するバイアスを効果的に除去することを示すための実験的および理論的な証拠を提供する。我々の実験は、我々の提案する評価指標である「期待値調整distinct（EAD）」が、応答の多様性を評価するための人間の判断とよりよく相関することを示している。将来の研究を支援するために、https://github.com/lsy641/Expectation-Adjusted-Distinctに例示する実装を提供する。"}
{"title": "How reparametrization trick broke differentially-private text representation learning", "url": "https://aclanthology.org/2022.acl-short.87/", "abstract": "As privacy gains traction in the NLP community, researchers have started adopting various approaches to privacy-preserving methods. One of the favorite privacy frameworks, differential privacy (DP), is perhaps the most compelling thanks to its fundamental theoretical guarantees. Despite the apparent simplicity of the general concept of differential privacy, it seems non-trivial to get it right when applying it to NLP. In this short paper, we formally analyze several recent NLP papers proposing text representation learning using DPText (Beigi et al., 2019a,b; Alnasser et al., 2021; Beigi et al., 2021) and reveal their false claims of being differentially private. Furthermore, we also show a simple yet general empirical sanity check to determine whether a given implementation of a DP mechanism almost certainly violates the privacy loss guarantees. Our main goal is to raise awareness and help the community understand potential pitfalls of applying differential privacy to text representation learning.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "再パラメータ化トリックが微分プライバシーを破ったテキスト表現学習について", "jabstract": "プライバシーがNLPコミュニティで注目されるにつれ、研究者たちはプライバシー保護手法を採用し始めています。お気に入りのプライバシーフレームワークの1つである差分プライバシー（DP）は、その基本的な理論的保証のおかげで、おそらく最も説得力があります。差分プライバシーの一般的な概念の明らかな単純さにもかかわらず、NLPに適用する際に正しく実装することは非常に困難であるようです。本論文では、DPText（Beigi et al.、2019a、b; Alnasser et al.、2021; Beigi et al.、2021）を使用したテキスト表現学習を提案する最近のいくつかのNLP論文を形式的に分析し、それらが差分プライバシーを満たしていないという誤った主張を明らかにします。さらに、与えられたDPメカニズムの実装がプライバシーロス保証をほぼ確実に違反しているかどうかを判断するための簡単で一般的な経験的な健全性チェックも示します。私たちの主な目標は、差分プライバシーをテキスト表現学習に適用する際の潜在的な落とし穴を理解し、コミュニティの意識を高めることです。"}
{"title": "Towards Consistent Document-level Entity Linking: Joint Models for Entity Linking and Coreference Resolution", "url": "https://aclanthology.org/2022.acl-short.88/", "abstract": "We consider the task of document-level entity linking (EL), where it is important to make consistent decisions for entity mentions over the full document jointly. We aim to leverage explicit “connections” among mentions within the document itself: we propose to join EL and coreference resolution (coref) in a single structured prediction task over directed trees and use a globally normalized model to solve it. This contrasts with related works where two separate models are trained for each of the tasks and additional logic is required to merge the outputs. Experimental results on two datasets show a boost of up to +5% F1-score on both coref and EL tasks, compared to their standalone counterparts. For a subset of hard cases, with individual mentions lacking the correct EL in their candidate entity list, we obtain a +50% increase in accuracy.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "一貫性のある文書レベルのエンティティリンキングに向けて：エンティティリンキングと共参照解決のためのジョイントモデル", "jabstract": "私たちは、ドキュメントレベルのエンティティリンキング（EL）のタスクを考慮しています。このタスクでは、ドキュメント全体にわたってエンティティ言及について一貫した決定を行うことが重要です。私たちは、ドキュメント内の言及間の明示的な「接続」を活用することを目的としています。つまり、有向木上の単一の構造化予測タスクでELと共参照解析（coref）を結合し、グローバルに正規化されたモデルを使用して解決することを提案します。これは、関連する作品とは対照的であり、それぞれのタスクに対して2つの別々のモデルがトレーニングされ、出力を統合するために追加のロジックが必要となります。2つのデータセットでの実験結果は、単独のコアフとELタスクに比べて、最大でF1スコアが+5％向上することを示しています。個々の言及が候補エンティティリストに正しいELを欠いている難しいケースのサブセットでは、正確性が+50％向上しました。"}
{"title": "A Flexible Multi-Task Model for BERT Serving", "url": "https://aclanthology.org/2022.acl-short.89/", "abstract": "We present an efficient BERT-based multi-task (MT) framework that is particularly suitable for iterative and incremental development of the tasks. The proposed framework is based on the idea of partial fine-tuning, i.e. only fine-tune some top layers of BERT while keep the other layers frozen. For each task, we train independently a single-task (ST) model using partial fine-tuning. Then we compress the task-specific layers in each ST model using knowledge distillation. Those compressed ST models are finally merged into one MT model so that the frozen layers of the former are shared across the tasks. We exemplify our approach on eight GLUE tasks, demonstrating that it is able to achieve 99.6% of the performance of the full fine-tuning method, while reducing up to two thirds of its overhead.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "BERT Servingのための柔軟なマルチタスクモデル", "jabstract": "私たちは、イテレーションとインクリメンタルなタスクの開発に特に適した効率的なBERTベースのマルチタスク（MT）フレームワークを提案します。提案されたフレームワークは、部分的なファインチューニングのアイデアに基づいています。つまり、BERTの一部の上位層のみをファインチューニングし、他の層を凍結します。各タスクについて、部分的なファインチューニングを使用して単一タスク（ST）モデルを独立してトレーニングします。その後、知識蒸留を使用して、各STモデルのタスク固有の層を圧縮します。これらの圧縮されたSTモデルは最終的に1つのMTモデルにマージされます。前者の凍結された層はタスク間で共有されます。私たちは、8つのGLUEタスクで私たちのアプローチを例示し、完全なファインチューニング方法の99.6％のパフォーマンスを達成できることを示し、そのオーバーヘッドを3分の2まで削減できることを示します。"}
{"title": "Understanding Game-Playing Agents with Natural Language Annotations", "url": "https://aclanthology.org/2022.acl-short.90/", "abstract": "We present a new dataset containing 10K human-annotated games of Go and show how these natural language annotations can be used as a tool for model interpretability. Given a board state and its associated comment, our approach uses linear probing to predict mentions of domain-specific terms (e.g., ko, atari) from the intermediate state representations of game-playing agents like AlphaGo Zero. We find these game concepts are nontrivially encoded in two distinct policy networks, one trained via imitation learning and another trained via reinforcement learning. Furthermore, mentions of domain-specific terms are most easily predicted from the later layers of both models, suggesting that these policy networks encode high-level abstractions similar to those used in the natural language annotations.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "自然言語注釈を用いたゲームプレイエージェントの理解", "jabstract": "私たちは、10,000の人間によるアノテーションが付けられた囲碁のゲームを含む新しいデータセットを提供し、これらの自然言語アノテーションがモデルの解釈可能性のツールとして使用できることを示します。ボードの状態とそれに関連するコメントが与えられた場合、私たちのアプローチは、AlphaGo Zeroのようなゲームプレイエージェントの中間状態表現から、ドメイン固有の用語（例：コウ、アタリ）の言及を予測するために線形プロービングを使用します。私たちは、これらのゲームの概念が、模倣学習によって訓練された1つのポリシーネットワークと、強化学習によって訓練された別のポリシーネットワークの2つの異なるポリシーネットワークに非自明にエンコードされていることを発見しました。さらに、ドメイン固有の用語の言及は、両方のモデルの後のレイヤーから最も簡単に予測されるため、これらのポリシーネットワークは、自然言語アノテーションで使用されるものに類似した高レベルの抽象化をエンコードしていることを示唆しています。"}
{"title": "Code Synonyms Do Matter: Multiple Synonyms Matching Network for Automatic ICD Coding", "url": "https://aclanthology.org/2022.acl-short.91/", "abstract": "Automatic ICD coding is defined as assigning disease codes to electronic medical records (EMRs).Existing methods usually apply label attention with code representations to match related text snippets.Unlike these works that model the label with the code hierarchy or description, we argue that the code synonyms can provide more comprehensive knowledge based on the observation that the code expressions in EMRs vary from their descriptions in ICD. By aligning codes to concepts in UMLS, we collect synonyms of every code. Then, we propose a multiple synonyms matching network to leverage synonyms for better code representation learning, and finally help the code classification. Experiments on the MIMIC-III dataset show that our proposed method outperforms previous state-of-the-art methods.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "コードの同義語は重要: 自動ICDコーディングのための複数の同義語マッチングネットワーク", "jabstract": "自動ICDコーディングは、電子医療記録（EMR）に疾患コードを割り当てることを定義しています。既存の方法は、通常、コード表現を使用して関連するテキストスニペットにラベルアテンションを適用します。コード階層または説明でラベルをモデル化するこれらの作品とは異なり、EMRのコード表現がICDの説明と異なることに基づいて、コードの同義語がより包括的な知識を提供できると主張します。コードをUMLSの概念に整列させることで、すべてのコードの同義語を収集します。次に、複数の同義語マッチングネットワークを提案して、同義語を活用してより良いコード表現学習を行い、最終的にコード分類を支援します。MIMIC-IIIデータセットでの実験は、提案された方法が以前の最先端の方法を上回ることを示しています。"}
{"title": "CoDA21: Evaluating Language Understanding Capabilities of NLP Models With Context-Definition Alignment", "url": "https://aclanthology.org/2022.acl-short.92/", "abstract": "Pretrained language models (PLMs) have achieved superhuman performance on many benchmarks, creating a need for harder tasks. We introduce CoDA21 (Context Definition Alignment), a challenging benchmark that measures natural language understanding (NLU) capabilities of PLMs: Given a definition and a context each for k words, but not the words themselves, the task is to align the k definitions with the k contexts. CoDA21 requires a deep understanding of contexts and definitions, including complex inference and world knowledge. We find that there is a large gap between human and PLM performance, suggesting that CoDA21 measures an aspect of NLU that is not sufficiently covered in existing benchmarks.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "CoDA21：コンテキスト定義アラインメントによるNLPモデルの言語理解能力の評価", "jabstract": "事前学習言語モデル（PLMs）は、多くのベンチマークで超人的なパフォーマンスを達成し、より難しいタスクが必要とされています。本論文では、CoDA21（Context Definition Alignment）を紹介し、PLMsの自然言語理解（NLU）能力を測定する難しいベンチマークとして提示します。k語の定義と文脈が与えられますが、単語自体は与えられず、kの定義をkの文脈に合わせるタスクです。CoDA21は、複雑な推論や世界知識を含む文脈と定義の深い理解を必要とします。人間とPLMのパフォーマンスには大きな差があり、CoDA21は既存のベンチマークでは十分にカバーされていないNLUの側面を測定していることを示唆しています。"}
{"title": "On the Importance of Effectively Adapting Pretrained Language Models for Active Learning", "url": "https://aclanthology.org/2022.acl-short.93/", "abstract": "Recent active learning (AL) approaches in Natural Language Processing (NLP) proposed using off-the-shelf pretrained language models (LMs). In this paper, we argue that these LMs are not adapted effectively to the downstream task during AL and we explore ways to address this issue. We suggest to first adapt the pretrained LM to the target task by continuing training with all the available unlabeled data and then use it for AL. We also propose a simple yet effective fine-tuning method to ensure that the adapted LM is properly trained in both low and high resource scenarios during AL. Our experiments demonstrate that our approach provides substantial data efficiency improvements compared to the standard fine-tuning approach, suggesting that a poor training strategy can be catastrophic for AL.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "自然言語処理に関する論文の要旨を以下に示します。日本語に翻訳してください。\n\n事前学習済み言語モデルを能動学習に効果的に適応することの重要性について", "jabstract": "自然言語処理（NLP）における最近のアクティブラーニング（AL）アプローチでは、市販の事前学習済み言語モデル（LM）を使用することが提案されています。本論文では、これらのLMがAL中に下流タスクに効果的に適応されていないことを主張し、この問題に対処する方法を探求します。まず、利用可能なすべての未ラベルデータを使用して、事前学習済みLMをターゲットタスクに適応させ、その後ALに使用することを提案します。また、適応されたLMがAL中の低リソースおよび高リソースのシナリオで適切にトレーニングされるようにするためのシンプルで効果的なファインチューニング方法を提案します。実験により、標準的なファインチューニングアプローチと比較して、私たちのアプローチがデータ効率性の改善を提供することが示され、トレーニング戦略が不十分であるということがALにとって壊滅的である可能性があることを示唆しています。"}
{"title": "A Recipe for Arbitrary Text Style Transfer with Large Language Models", "url": "https://aclanthology.org/2022.acl-short.94/", "abstract": "In this paper, we leverage large language models (LLMs) to perform zero-shot text style transfer. We present a prompting method that we call augmented zero-shot learning, which frames style transfer as a sentence rewriting task and requires only a natural language instruction, without model fine-tuning or exemplars in the target style. Augmented zero-shot learning is simple and demonstrates promising results not just on standard style transfer tasks such as sentiment, but also on arbitrary transformations such as ‘make this melodramatic’ or ‘insert a metaphor.’", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "大規模言語モデルを用いた任意のテキストスタイル転送のレシピ", "jabstract": "この論文では、大規模言語モデル（LLM）を活用して、ゼロショットテキストスタイル変換を実行します。私たちは、スタイル変換を文の書き換えタスクとしてフレーム化し、モデルの微調整やターゲットスタイルの例示なしに、自然言語の指示だけで実行できる拡張ゼロショット学習と呼ぶプロンプティング方法を提案します。拡張ゼロショット学習はシンプルで、感情などの標準的なスタイル変換タスクだけでなく、「これをメロドラマティックにする」や「比喩を挿入する」などの任意の変換に対しても有望な結果を示します。"}
{"title": "DiS-ReX: A Multilingual Dataset for Distantly Supervised Relation Extraction", "url": "https://aclanthology.org/2022.acl-short.95/", "abstract": "Our goal is to study the novel task of distant supervision for multilingual relation extraction (Multi DS-RE). Research in Multi DS-RE has remained limited due to the absence of a reliable benchmarking dataset. The only available dataset for this task, RELX-Distant (Köksal and Özgür, 2020), displays several unrealistic characteristics, leading to a systematic overestimation of model performance. To alleviate these concerns, we release a new benchmark dataset for the task, named DiS-ReX. We also modify the widely-used bag attention models using an mBERT encoder and provide the first baseline results on the proposed task. We show that DiS-ReX serves as a more challenging dataset than RELX-Distant, leaving ample room for future research in this domain.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "DiS-ReX：遠隔監視された関係抽出のための多言語データセット", "jabstract": "私たちの目標は、多言語関係抽出の遠隔監視という新しいタスク（Multi DS-RE）を研究することです。Multi DS-REの研究は、信頼できるベンチマークデータセットの欠如により限定されています。このタスクの唯一の利用可能なデータセットであるRELX-Distant（Köksal and Özgür, 2020）は、いくつかの非現実的な特徴を示しており、モデルの性能を系統的に過大評価しています。これらの懸念を軽減するために、私たちはDiS-ReXという新しいベンチマークデータセットを提供します。また、mBERTエンコーダを使用して広く使用されているバッグアテンションモデルを修正し、提案されたタスクの最初のベースライン結果を提供します。DiS-ReXはRELX-Distantよりもより難しいデータセットとして機能し、この領域での将来の研究に十分な余地を残します。"}
{"title": "(Un)solving Morphological Inflection: Lemma Overlap Artificially Inflates Models’ Performance", "url": "https://aclanthology.org/2022.acl-short.96/", "abstract": "In the domain of Morphology, Inflection is a fundamental and important task that gained a lot of traction in recent years, mostly via SIGMORPHON’s shared-tasks.With average accuracy above 0.9 over the scores of all languages, the task is considered mostly solved using relatively generic neural seq2seq models, even with little data provided.In this work, we propose to re-evaluate morphological inflection models by employing harder train-test splits that will challenge the generalization capacity of the models. In particular, as opposed to the naïve split-by-form, we propose a split-by-lemma method to challenge the performance on existing benchmarks.Our experiments with the three top-ranked systems on the SIGMORPHON’s 2020 shared-task show that the lemma-split presents an average drop of 30 percentage points in macro-average for the 90 languages included. The effect is most significant for low-resourced languages with a drop as high as 95 points, but even high-resourced languages lose about 10 points on average. Our results clearly show that generalizing inflection to unseen lemmas is far from being solved, presenting a simple yet effective means to promote more sophisticated models.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "形態論的屈折の(非)解決：基本形の重複がモデルの性能を人工的に高める\n\nA Comprehensive Study of Named Entity Recognition in Japanese: State-of-the-Art Models and Future Directions\n\n日本語における固有表現認識の包括的研究：最新のモデルと今後の方向性", "jabstract": "形態論の領域において、屈折は重要で基本的なタスクであり、SIGMORPHONの共有タスクを通じて最近注目を集めています。全言語のスコアに対する平均精度が0.9以上であるため、比較的一般的なニューラルseq2seqモデルを使用して、タスクはほとんど解決されたと考えられています。本研究では、モデルの一般化能力に挑戦するより困難なトレイン-テスト分割を採用することで、形態的屈折モデルを再評価することを提案します。特に、形態素による分割ではなく、既存のベンチマークのパフォーマンスに挑戦するために、基本形による分割を提案します。SIGMORPHONの2020年共有タスクで上位3つのシステムを用いた実験により、90言語全体のマクロ平均において、基本形による分割により平均30パーセントの低下が見られました。この効果は、低リソース言語においては95ポイントまで低下することが最も顕著であり、高リソース言語でも平均で10ポイントの低下が見られます。私たちの結果は、未知の基本形に対する屈折の一般化が解決されていないことを明確に示しており、より洗練されたモデルを促進するための簡単で効果的な手段を提供しています。"}
{"title": "Text Smoothing: Enhance Various Data Augmentation Methods on Text Classification Tasks", "url": "https://aclanthology.org/2022.acl-short.97/", "abstract": "Before entering the neural network, a token needs to be converted to its one-hot representation, which is a discrete distribution of the vocabulary. Smoothed representation is the probability of candidate tokens obtained from the pre-trained masked language model, which can be seen as a more informative augmented substitution to the one-hot representation. We propose an efficient data augmentation method, dub as text smoothing, by converting a sentence from its one-hot representation to controllable smoothed representation.We evaluate text smoothing on different datasets in a low-resource regime. Experimental results show that text smoothing outperforms various mainstream data augmentation methods by a substantial margin. Moreover, text smoothing can be combined with these data augmentation methods to achieve better performance.", "vol-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "jtitle": "テキストスムージング：テキスト分類タスクにおける様々なデータ拡張手法を強化する", "jabstract": "ニューラルネットワークに入力する前に、トークンはそのボキャブラリーの離散分布であるワンホット表現に変換する必要があります。スムーズ化された表現は、事前学習されたマスクされた言語モデルから得られた候補トークンの確率であり、より情報量の多い拡張された代替と見なすことができます。我々は、文をワンホット表現から制御可能なスムーズ化された表現に変換する、テキストスムージングと呼ばれる効率的なデータ拡張方法を提案します。我々は、低リソース環境で異なるデータセットでテキストスムージングを評価しました。実験結果は、テキストスムージングが、様々な主流のデータ拡張方法よりも大幅に優れていることを示しています。さらに、テキストスムージングはこれらのデータ拡張方法と組み合わせて、より良い性能を発揮することができます。"}
